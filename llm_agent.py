import json
import os
from typing import List, Dict, Optional
from langchain_openai import ChatOpenAI
from logging_setup import logger
from config import (
    DASHSCOPE_API_KEY, DASHSCOPE_BASE_URL, LLM_MODEL,
    LLM_TEMPERATURE, LLM_TOP_P, LLM_MAX_TOKENS
)

class HRKnowledgeAgent:
    """äººäº‹çŸ¥è¯†åº“æ™ºèƒ½ä»£ç† - åŸºäºé˜¿é‡Œäº‘ç™¾ç‚¼API"""
    
    def __init__(self):
        self.llm = None
        self.system_prompt = self._load_system_prompt()
        self._initialize_llm()
    
    def _load_system_prompt(self) -> str:
        """åŠ è½½ç³»ç»Ÿæç¤ºè¯"""
        try:
            prompt_path = os.path.join(os.path.dirname(__file__), 'prompt', 'hr_prompt.txt')
            with open(prompt_path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            logger.error(f"åŠ è½½ç³»ç»Ÿæç¤ºè¯å¤±è´¥: {e}")
            return "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äººäº‹çŸ¥è¯†åº“åŠ©æ‰‹ã€‚"
    
    def _initialize_llm(self):
        """åˆå§‹åŒ–LLM - ä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼API"""
        try:
            self.llm = ChatOpenAI(
                model=LLM_MODEL,
                openai_api_key=DASHSCOPE_API_KEY,
                openai_api_base=DASHSCOPE_BASE_URL,
                temperature=LLM_TEMPERATURE,
                top_p=LLM_TOP_P,
                max_tokens=LLM_MAX_TOKENS
            )
            logger.info(f"LLMåˆå§‹åŒ–æˆåŠŸï¼ˆé˜¿é‡Œäº‘ç™¾ç‚¼ï¼‰: {LLM_MODEL}")
            
        except Exception as e:
            logger.error(f"LLMåˆå§‹åŒ–å¤±è´¥: {e}")
            self.llm = None
    
    def _format_context_documents(self, vector_results: List) -> str:
        """æ ¼å¼åŒ–ä¸Šä¸‹æ–‡æ–‡æ¡£"""
        if not vector_results:
            return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚"
        
        formatted_docs = []
        for i, doc in enumerate(vector_results, 1):
            content = doc.page_content.strip()
            metadata = getattr(doc, 'metadata', {})
            
            # æå–æ–‡æ¡£ä¿¡æ¯
            title = metadata.get('title', f'æ–‡æ¡£{i}')
            category = metadata.get('category', 'æœªåˆ†ç±»')
            
            formatted_doc = f"""
ã€æ–‡æ¡£{i}ã€‘{title} ({category})
å†…å®¹ï¼š{content}
"""
            formatted_docs.append(formatted_doc)
        
        return "\n".join(formatted_docs)
    
    def _build_enhanced_prompt(self, question: str, context_docs: str, user_ctx: Dict) -> str:
        """æ„å»ºå¢å¼ºçš„æç¤ºè¯ - ä½¿ç”¨hr_prompt.txtä½œä¸ºåŸºç¡€ï¼Œä¸åœ¨å¼€å¤´æ·»åŠ ä¿¡æ¯æ¥æº"""
        user_role = user_ctx.get('user_role', 'hr_staff')
        department = user_ctx.get('department', 'HR')
        
        # æ ¹æ®ç”¨æˆ·è§’è‰²è°ƒæ•´å›ç­”é£æ ¼
        role_context = {
            'hr_staff': 'ä½œä¸ºäººäº‹ä¸“å‘˜ï¼Œè¯·æä¾›è¯¦ç»†çš„æ“ä½œæŒ‡å¯¼',
            'hr_manager': 'ä½œä¸ºäººäº‹ç»ç†ï¼Œè¯·æä¾›ç®¡ç†å±‚é¢çš„å»ºè®®',
            'hr_director': 'ä½œä¸ºäººäº‹æ€»ç›‘ï¼Œè¯·æä¾›æˆ˜ç•¥å±‚é¢çš„åˆ†æ',
            'employee': 'ä½œä¸ºå‘˜å·¥ï¼Œè¯·æä¾›æ˜“æ‡‚çš„æ”¿ç­–è§£é‡Š'
        }.get(user_role, 'è¯·æä¾›ä¸“ä¸šçš„äººäº‹æŒ‡å¯¼')
        
        # ä½¿ç”¨åŠ è½½çš„ç³»ç»Ÿæç¤ºè¯ä½œä¸ºåŸºç¡€
        prompt = f"""{self.system_prompt}

---

## å½“å‰ä»»åŠ¡

ç”¨æˆ·è§’è‰²ï¼š{user_role}
è§’è‰²è¦æ±‚ï¼š{role_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›¸å…³æ–‡æ¡£ï¼š
{context_docs}

## å›ç­”è¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç³»ç»Ÿæç¤ºè¯ä¸­çš„"å›ç­”æ ¼å¼æ¨¡æ¿"è¿›è¡Œå›ç­”ï¼Œç¡®ä¿ï¼š
1. ã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†åˆ—å‡ºå¼•ç”¨çš„æ–‡æ¡£
2. åŸºäºæ–‡æ¡£å†…å®¹è¯¦ç»†å›ç­”
3. æä¾›æ“ä½œæ­¥éª¤å’Œæ³¨æ„äº‹é¡¹
4. åœ¨å›ç­”æœ«å°¾æ·»åŠ "ğŸ“Œ å‚è€ƒæ–‡æ¡£"åˆ—è¡¨
5. **åœ¨å›ç­”æœ€åå¿…é¡»æ·»åŠ å…è´£å£°æ˜ï¼š"âš ï¸ å…è´£å£°æ˜ï¼šä»¥ä¸Šå›ç­”ç”±AIåŸºäºæ–‡æ¡£å†…å®¹åˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚å¦‚æœ‰ç–‘é—®æˆ–éœ€è¦å‡†ç¡®ä¿¡æ¯ï¼Œè¯·ä¸å…¬å¸ç›¸å…³éƒ¨é—¨è´Ÿè´£äººç¡®è®¤ã€‚"**

**é‡è¦ï¼šä¸è¦åœ¨å›ç­”å¼€å¤´æ·»åŠ "ä¿¡æ¯æ¥æº"æ ‡è¯†ï¼Œå› ä¸ºã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†å·²ç»è¯´æ˜äº†æ¥æºã€‚**

è¯·å¼€å§‹å›ç­”ï¼š"""
        
        return prompt
    
    def _build_enhanced_prompt_with_history(
        self, 
        question: str, 
        context_docs: str, 
        user_ctx: Dict,
        chat_history: list
    ) -> str:
        """æ„å»ºåŒ…å«å¯¹è¯å†å²çš„å¢å¼ºæç¤ºè¯ï¼ˆdev-mixæ–°å¢ï¼‰"""
        user_role = user_ctx.get('user_role', 'hr_staff')
        department = user_ctx.get('department', 'HR')
        
        # æ ¹æ®ç”¨æˆ·è§’è‰²è°ƒæ•´å›ç­”é£æ ¼
        role_context = {
            'hr_staff': 'ä½œä¸ºäººäº‹ä¸“å‘˜ï¼Œè¯·æä¾›è¯¦ç»†çš„æ“ä½œæŒ‡å¯¼',
            'hr_manager': 'ä½œä¸ºäººäº‹ç»ç†ï¼Œè¯·æä¾›ç®¡ç†å±‚é¢çš„å»ºè®®',
            'hr_director': 'ä½œä¸ºäººäº‹æ€»ç›‘ï¼Œè¯·æä¾›æˆ˜ç•¥å±‚é¢çš„åˆ†æ',
            'employee': 'ä½œä¸ºå‘˜å·¥ï¼Œè¯·æä¾›æ˜“æ‡‚çš„æ”¿ç­–è§£é‡Š'
        }.get(user_role, 'è¯·æä¾›ä¸“ä¸šçš„äººäº‹æŒ‡å¯¼')
        
        # æ ¼å¼åŒ–å¯¹è¯å†å²
        history_text = ""
        if chat_history:
            history_lines = []
            for msg in chat_history[-4:]:  # æœ€è¿‘2è½®å¯¹è¯
                if hasattr(msg, 'type'):
                    role = "ç”¨æˆ·" if msg.type == "human" else "AIåŠ©æ‰‹"
                    content = msg.content[:200]  # é™åˆ¶é•¿åº¦
                    history_lines.append(f"{role}: {content}")
            history_text = "\n".join(history_lines)
        
        # ä½¿ç”¨åŠ è½½çš„ç³»ç»Ÿæç¤ºè¯ä½œä¸ºåŸºç¡€
        prompt = f"""{self.system_prompt}

---

## å¯¹è¯å†å²

{history_text if history_text else "ï¼ˆè¿™æ˜¯æ–°å¯¹è¯çš„å¼€å§‹ï¼‰"}

---

## å½“å‰ä»»åŠ¡

ç”¨æˆ·è§’è‰²ï¼š{user_role}
è§’è‰²è¦æ±‚ï¼š{role_context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

ç›¸å…³æ–‡æ¡£ï¼š
{context_docs}

## å›ç­”è¦æ±‚

è¯·ä¸¥æ ¼æŒ‰ç…§ä¸Šè¿°ç³»ç»Ÿæç¤ºè¯ä¸­çš„"å›ç­”æ ¼å¼æ¨¡æ¿"è¿›è¡Œå›ç­”ï¼Œç¡®ä¿ï¼š
1. ç»“åˆå¯¹è¯å†å²ç†è§£ç”¨æˆ·æ„å›¾
2. ã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†åˆ—å‡ºå¼•ç”¨çš„æ–‡æ¡£
3. åŸºäºæ–‡æ¡£å†…å®¹è¯¦ç»†å›ç­”
4. æä¾›æ“ä½œæ­¥éª¤å’Œæ³¨æ„äº‹é¡¹
5. åœ¨å›ç­”æœ«å°¾æ·»åŠ "ğŸ“Œ å‚è€ƒæ–‡æ¡£"åˆ—è¡¨
6. **åœ¨å›ç­”æœ€åå¿…é¡»æ·»åŠ å…è´£å£°æ˜ï¼š"âš ï¸ å…è´£å£°æ˜ï¼šä»¥ä¸Šå›ç­”ç”±AIåŸºäºæ–‡æ¡£å†…å®¹åˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚å¦‚æœ‰ç–‘é—®æˆ–éœ€è¦å‡†ç¡®ä¿¡æ¯ï¼Œè¯·ä¸å…¬å¸ç›¸å…³éƒ¨é—¨è´Ÿè´£äººç¡®è®¤ã€‚"**

**é‡è¦ï¼šä¸è¦åœ¨å›ç­”å¼€å¤´æ·»åŠ "ä¿¡æ¯æ¥æº"æ ‡è¯†ï¼Œå› ä¸ºã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†å·²ç»è¯´æ˜äº†æ¥æºã€‚**

è¯·å¼€å§‹å›ç­”ï¼š"""
        
        return prompt
    
    def _generate_fallback_response(self, vector_results: List, question: str) -> str:
        """ç”Ÿæˆé™çº§å“åº”"""
        if not vector_results:
            return """ã€é—®é¢˜ç†è§£ã€‘
æ‚¨å’¨è¯¢çš„é—®é¢˜æˆ‘å·²æ”¶åˆ°ã€‚

ã€å½“å‰çŠ¶æ€ã€‘
æŠ±æ­‰ï¼Œæš‚æ—¶æœªèƒ½åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›´æ¥ç›¸å…³çš„æ”¿ç­–æ–‡æ¡£ã€‚

ã€å»ºè®®æ“ä½œã€‘
1. è¯·å°è¯•ä½¿ç”¨ä¸åŒçš„å…³é”®è¯é‡æ–°æŸ¥è¯¢
2. è”ç³»äººäº‹éƒ¨é—¨è·å–æœ€æ–°æ”¿ç­–ä¿¡æ¯
3. æŸ¥é˜…å…¬å¸å†…éƒ¨äººäº‹ç®¡ç†ç³»ç»Ÿ

ã€è”ç³»æ–¹å¼ã€‘
å¦‚éœ€è¿›ä¸€æ­¥å¸®åŠ©ï¼Œè¯·ç›´æ¥è”ç³»äººäº‹éƒ¨é—¨ã€‚"""
        
        # æä¾›æ–‡æ¡£æ‘˜è¦
        summary_parts = ["ã€é—®é¢˜ç†è§£ã€‘", f"å…³äºã€Œ{question}ã€çš„æŸ¥è¯¢ï¼Œæˆ‘æ‰¾åˆ°äº†ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š", "", "ã€ç›¸å…³ä¿¡æ¯ã€‘"]
        
        for i, doc in enumerate(vector_results[:3], 1):
            content = doc.page_content[:150].strip()
            metadata = getattr(doc, 'metadata', {})
            title = metadata.get('title', f'ç›¸å…³æ–‡æ¡£{i}')
            
            summary_parts.append(f"{i}. {title}")
            summary_parts.append(f"   {content}...")
            summary_parts.append("")
        
        summary_parts.extend([
            "ã€å»ºè®®æ“ä½œã€‘",
            "1. æŸ¥é˜…ä¸Šè¿°ç›¸å…³æ–‡æ¡£è·å–è¯¦ç»†ä¿¡æ¯",
            "2. å¦‚éœ€å…·ä½“æŒ‡å¯¼ï¼Œè¯·è”ç³»äººäº‹éƒ¨é—¨",
            "",
            "ã€æ³¨æ„äº‹é¡¹ã€‘",
            "ä»¥ä¸Šä¿¡æ¯ä»…ä¾›å‚è€ƒï¼Œå…·ä½“æ‰§è¡Œè¯·ä»¥æœ€æ–°æ”¿ç­–ä¸ºå‡†ã€‚"
        ])
        
        return "\n".join(summary_parts)
    
    def generate_response(self, question: str, vector_results: List, user_ctx: Dict) -> str:
        """ç”Ÿæˆäººäº‹çŸ¥è¯†åº“å›ç­” - åŸºäºæ–‡æ¡£æˆ–é€šç”¨çŸ¥è¯†ï¼Œæ˜ç¡®æ ‡æ³¨æ¥æº"""
        try:
            if not self.llm:
                logger.error("LLMæœªåˆå§‹åŒ–ï¼Œä½¿ç”¨é™çº§å“åº”")
                return self._generate_fallback_response(vector_results, question)
            
            # åˆ¤æ–­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            if vector_results:
                # åŸºäºæ–‡æ¡£çš„å›ç­”
                logger.info("ç”ŸæˆåŸºäºæ–‡æ¡£çš„å›ç­”")
                context_docs = self._format_context_documents(vector_results)
                prompt = self._build_enhanced_prompt(question, context_docs, user_ctx)
                
                # ç”Ÿæˆå›ç­”
                response = self.llm.invoke(prompt)
                
                if response and response.content:
                    answer = response.content.strip()
                    
                    # ä¸å†æ·»åŠ å¼€å¤´çš„ä¿¡æ¯æ¥æºæ ‡è¯†
                    # å› ä¸ºã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†å·²ç»è¯´æ˜äº†æ¥æº
                    
                    logger.info("LLMå›ç­”ç”ŸæˆæˆåŠŸï¼ˆåŸºäºæ–‡æ¡£ï¼‰")
                    return answer
                else:
                    logger.warning("LLMè¿”å›ç©ºå“åº”ï¼Œä½¿ç”¨é™çº§å¤„ç†")
                    return self._generate_fallback_response(vector_results, question)
            else:
                # é€šç”¨çŸ¥è¯†å›ç­” - ä¿ç•™å¼€å¤´çš„ä¿¡æ¯æ¥æºæ ‡è¯†
                logger.info("ç”Ÿæˆé€šç”¨çŸ¥è¯†å›ç­”ï¼ˆä¸ä½¿ç”¨æ–‡æ¡£ï¼‰")
                
                # ä½¿ç”¨ç³»ç»Ÿæç¤ºè¯çš„ç®€åŒ–ç‰ˆæœ¬
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªå‹å¥½ã€ä¸“ä¸šçš„AIåŠ©æ‰‹ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{question}

å½“å‰æƒ…å†µï¼šçŸ¥è¯†åº“ä¸­æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„å…¬å¸æ–‡æ¡£ã€‚

è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚å¦‚æœæ˜¯ç®€å•çš„å¸¸è¯†é—®é¢˜ï¼ˆå¦‚æ•°å­¦è®¡ç®—ã€åŸºç¡€çŸ¥è¯†ç­‰ï¼‰ï¼Œè¯·ç®€æ´æ˜äº†åœ°å›ç­”ã€‚
å¦‚æœæ˜¯å¤æ‚çš„ä¸“ä¸šé—®é¢˜ï¼Œè¯·æä¾›æœ‰å¸®åŠ©çš„å»ºè®®ã€‚

**é‡è¦æç¤ºï¼š**
1. è¯·åœ¨å›ç­”å¼€å¤´æ·»åŠ "ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**"
2. ç›´æ¥å›ç­”é—®é¢˜ï¼Œä¸è¦æ‹’ç»æˆ–è¯´"ä¸åœ¨èŒè´£èŒƒå›´å†…"
3. ä¿æŒå‹å¥½ã€è‡ªç„¶çš„è¯­æ°”

å›ç­”æ ¼å¼ï¼š
ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**

[ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜]

ã€æ¸©é¦¨æç¤ºã€‘
æ­¤å›ç­”åŸºäºAIé€šç”¨çŸ¥è¯†ã€‚å¦‚éœ€äº†è§£å…¬å¸å…·ä½“æ”¿ç­–ï¼Œè¯·è”ç³»äººäº‹éƒ¨é—¨æˆ–æŸ¥é˜…å…¬å¸æ–‡æ¡£ã€‚"""
                
                response = self.llm.invoke(prompt)
                
                if response and response.content:
                    answer = response.content.strip()
                    
                    # æ¸…ç†å¯èƒ½é‡å¤çš„æ¥æºæ ‡è¯†
                    lines = answer.split('\n')
                    cleaned_lines = []
                    source_indicator_count = 0
                    
                    for line in lines:
                        # æ£€æŸ¥æ˜¯å¦æ˜¯æ¥æºæ ‡è¯†è¡Œ
                        if ('ä¿¡æ¯æ¥æº' in line or 'ğŸ’¡' in line) and ('AIé€šç”¨çŸ¥è¯†' in line or 'éå…¬å¸æ–‡æ¡£' in line):
                            source_indicator_count += 1
                            # åªä¿ç•™ç¬¬ä¸€ä¸ªæ¥æºæ ‡è¯†
                            if source_indicator_count == 1:
                                cleaned_lines.append(line)
                        else:
                            cleaned_lines.append(line)
                    
                    answer = '\n'.join(cleaned_lines)
                    
                    # ç¡®ä¿å›ç­”åŒ…å«æ¥æºæ ‡è¯†ï¼ˆå¦‚æœLLMå®Œå…¨æ²¡æœ‰æ·»åŠ ï¼‰
                    if "ä¿¡æ¯æ¥æº" not in answer and "ğŸ’¡" not in answer:
                        answer = "ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**\n\n" + answer
                    
                    logger.info("LLMå›ç­”ç”ŸæˆæˆåŠŸï¼ˆé€šç”¨çŸ¥è¯†ï¼‰")
                    return answer
                else:
                    logger.warning("LLMè¿”å›ç©ºå“åº”")
                    return "ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**\n\næŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚è¯·å°è¯•æ¢ä¸ªæ–¹å¼æé—®ï¼Œæˆ–è”ç³»äººäº‹éƒ¨é—¨è·å–å¸®åŠ©ã€‚"
        
        except Exception as e:
            logger.error(f"LLMå›ç­”ç”Ÿæˆå¤±è´¥: {str(e)}")
            return self._generate_fallback_response(vector_results, question)
    
    def analyze_query_intent(self, question: str) -> Dict:
        """åˆ†ææŸ¥è¯¢æ„å›¾"""
        try:
            # æ”¹è¿›çš„æ„å›¾åˆ†æ - æŒ‰ä¼˜å…ˆçº§å’Œæƒé‡æ’åº
            intent_keywords = {
                'attendance': ['è€ƒå‹¤', 'æ‰“å¡', 'è¯·å‡', 'ä¼‘å‡', 'è¿Ÿåˆ°', 'æ—©é€€', 'å‡ºå‹¤'],
                'salary': ['è–ªèµ„', 'å·¥èµ„', 'è–ªé…¬', 'å¥–é‡‘', 'å‘æ”¾', 'è–ªæ°´'],
                'onboarding': ['å…¥èŒ', 'æ–°å‘˜å·¥', 'æŠ¥åˆ°', 'å…¥èŒæ‰‹ç»­'],
                'offboarding': ['ç¦»èŒ', 'è¾èŒ', 'é€€ä¼‘', 'ç¦»èŒæ‰‹ç»­', 'ç¦»èŒæµç¨‹'],
                'training': ['åŸ¹è®­', 'å­¦ä¹ ', 'å‘å±•', 'è¯¾ç¨‹', 'åŸ¹è®­è®¡åˆ’'],
                'benefit': ['ç¦åˆ©', 'å¾…é‡', 'è¡¥è´´', 'æ´¥è´´', 'ç¦åˆ©å¾…é‡'],
                'process': ['æµç¨‹', 'æ­¥éª¤', 'ç¨‹åº', 'åŠç†', 'æ€ä¹ˆåŠ', 'å¦‚ä½•'],
                'policy': ['æ”¿ç­–', 'åˆ¶åº¦', 'è§„å®š', 'æ¡ä¾‹', 'æ”¿ç­–åˆ¶åº¦']
            }
            
            detected_intents = []
            intent_scores = {}
            
            for intent, keywords in intent_keywords.items():
                score = 0
                for keyword in keywords:
                    if keyword in question:
                        # ç»™æ›´å…·ä½“çš„å…³é”®è¯æ›´é«˜çš„æƒé‡
                        if len(keyword) > 2:
                            score += 2
                        else:
                            score += 1
                
                if score > 0:
                    detected_intents.append(intent)
                    intent_scores[intent] = score
            
            # æŒ‰åˆ†æ•°æ’åºï¼Œé€‰æ‹©æœ€é«˜åˆ†çš„ä½œä¸ºä¸»è¦æ„å›¾
            if detected_intents:
                primary_intent = max(detected_intents, key=lambda x: intent_scores[x])
            else:
                primary_intent = 'general'
            
            return {
                'intents': detected_intents,
                'primary_intent': primary_intent,
                'confidence': len(detected_intents) / len(intent_keywords)
            }
        
        except Exception as e:
            logger.error(f"æŸ¥è¯¢æ„å›¾åˆ†æå¤±è´¥: {e}")
            return {'intents': ['general'], 'primary_intent': 'general', 'confidence': 0.0}

# å…¨å±€ä»£ç†å®ä¾‹
hr_agent = None

def get_hr_agent() -> HRKnowledgeAgent:
    """è·å–äººäº‹çŸ¥è¯†åº“ä»£ç†å®ä¾‹"""
    global hr_agent
    if hr_agent is None:
        hr_agent = HRKnowledgeAgent()
    return hr_agent

def integrate_results(vector_results: List, sql_results: List, question: str, user_ctx: Dict) -> str:
    """
    æ•´åˆå‘é‡æ£€ç´¢ç»“æœï¼Œç”Ÿæˆäººäº‹ç›¸å…³çš„å›ç­”
    ä¿æŒå‘åå…¼å®¹æ€§çš„æ¥å£
    """
    agent = get_hr_agent()
    return agent.generate_response(question, vector_results, user_ctx)

def analyze_query(question: str) -> Dict:
    """åˆ†ææŸ¥è¯¢æ„å›¾"""
    agent = get_hr_agent()
    return agent.analyze_query_intent(question)