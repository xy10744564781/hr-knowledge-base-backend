import time
import json
import asyncio
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
from fastapi import HTTPException
from logging_setup import logger
from knowledge_base import init_chroma, process_upload_file
from knowledge_base import get_vector_manager
from schemas import (
    QueryRequest, QueryResponse, DocumentUploadResponse, 
    HealthResponse, VectorStoreStatus, UserContext, DocumentInfo,
    DocumentCategory, AccessLevel
)
from llm_agent import integrate_results, analyze_query, get_hr_agent
from typing import List, Optional, AsyncGenerator, Dict
from config import QUERY_TIMEOUT, MAX_QUERY_RESULTS

# dev-mixæ–°å¢ï¼šå¯¼å…¥å¯¹è¯å†å²ç®¡ç†å™¨
from chat_history_manager import get_history_manager, sync_history_to_manager

# åˆå§‹åŒ–å‘é‡åº“å’Œæ™ºèƒ½è·¯ç”±å™¨
try:
    # è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³protobufç‰ˆæœ¬å†²çªå’Œç¦ç”¨é¥æµ‹
    import os
    os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
    os.environ['ANONYMIZED_TELEMETRY'] = 'False'
    os.environ['CHROMA_TELEMETRY'] = 'False'
    
    from knowledge_base import init_chroma, get_vector_manager
    from query_router import create_query_router
    
    vector_store = init_chroma()
    vector_manager = get_vector_manager()
    
    # ä¿ç•™æ—§çš„æŸ¥è¯¢è·¯ç”±å™¨ä½œä¸ºé™çº§é€‰é¡¹
    legacy_query_router = create_query_router()
    
    # å°è¯•åˆå§‹åŒ–æ–°çš„æ™ºèƒ½è·¯ç”±å™¨
    intelligent_router = None
    try:
        from langchain_chains.intelligent_router import get_intelligent_router
        intelligent_router = get_intelligent_router()
        logger.info('æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨åˆå§‹åŒ–æˆåŠŸ')
    except Exception as router_error:
        logger.warning(f'æ™ºèƒ½è·¯ç”±å™¨åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿè·¯ç”±å™¨: {router_error}')
        intelligent_router = None
    
    # åˆå§‹åŒ–å“åº”ç”Ÿæˆå™¨ï¼ˆä¿ç•™ä½œä¸ºé™çº§é€‰é¡¹ï¼‰
    hr_agent = get_hr_agent()
    
    if vector_store:
        logger.info('ChromaDBå‘é‡åº“åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨LangChain + é˜¿é‡Œäº‘APIï¼‰')
        if intelligent_router:
            logger.info('ç³»ç»Ÿè¿è¡Œåœ¨æ™ºèƒ½å¤šéƒ¨é—¨æ¨¡å¼')
        else:
            logger.info('ç³»ç»Ÿè¿è¡Œåœ¨ä¼ ç»Ÿè·¯ç”±æ¨¡å¼')
    else:
        logger.warning('ChromaDBå‘é‡åº“åˆå§‹åŒ–å¤±è´¥ï¼Œç³»ç»Ÿå°†ä»¥ç®€åŒ–æ¨¡å¼è¿è¡Œ')
        
except Exception as e:
    logger.error(f'å‘é‡åº“åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}', exc_info=True)
    vector_store = None
    vector_manager = None
    legacy_query_router = None
    intelligent_router = None
    
    # ç®€åŒ–æ¨¡å¼ä¸‹çš„å“åº”ç”Ÿæˆå™¨
    hr_agent = get_hr_agent()

def service_query_knowledge(request: QueryRequest) -> QueryResponse:
    """å¤„ç†ä¼ä¸šçŸ¥è¯†æŸ¥è¯¢è¯·æ±‚ - ä½¿ç”¨æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨"""
    start_time = time.time()
    logger.info(f"æ”¶åˆ°ä¼ä¸šçŸ¥è¯†æŸ¥è¯¢: {request.question}")

    try:
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„æ™ºèƒ½è·¯ç”±å™¨
        if intelligent_router:
            logger.info("ä½¿ç”¨æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨å¤„ç†æŸ¥è¯¢")
            
            # ä½¿ç”¨æ™ºèƒ½è·¯ç”±å™¨å¤„ç†æŸ¥è¯¢
            query_result = intelligent_router.route_query(
                query=request.question,
                user_id=request.user_ctx.user_id,
                session_id=request.session_id
            )
            
            # è½¬æ¢ä¸ºQueryResponseæ ¼å¼
            response = QueryResponse(
                answer=query_result.answer,
                source_data=[{
                    "tool": "intelligent_multi_department_router",
                    "source_type": query_result.source_type,
                    "user_department": query_result.user_context.department,
                    "intent_analysis": {
                        "primary_intent": query_result.intent_analysis.primary_intent,
                        "confidence": query_result.intent_analysis.confidence,
                        "detected_department": query_result.intent_analysis.detected_department
                    },
                    "retrieval_strategy": {
                        "primary_folders": query_result.retrieval_strategy.primary_folders,
                        "secondary_folders": query_result.retrieval_strategy.secondary_folders
                    },
                    "documents": [
                        {
                            "title": doc.title,
                            "department": doc.department,
                            "score": doc.score,
                            "document_id": doc.document_id
                        }
                        for doc in query_result.documents
                    ]
                }],
                confidence=query_result.confidence,
                processing_time=query_result.processing_time
            )
            
            logger.info("æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨å¤„ç†å®Œæˆ")
            return response
        
        # é™çº§åˆ°æ—§çš„æŸ¥è¯¢è·¯ç”±å™¨
        elif legacy_query_router and vector_store:
            logger.warning("æ™ºèƒ½è·¯ç”±å™¨ä¸å¯ç”¨ï¼Œé™çº§åˆ°ä¼ ç»Ÿè·¯ç”±å™¨")
            return _legacy_query_processing(request, start_time)
        
        else:
            raise HTTPException(status_code=500, detail="æŸ¥è¯¢ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–")
            
    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
        return _handle_query_error(e, start_time)


def _legacy_query_processing(request: QueryRequest, start_time: float) -> QueryResponse:
    """ä¼ ç»ŸæŸ¥è¯¢å¤„ç†é€»è¾‘ï¼ˆé™çº§å¤„ç†ï¼‰"""
    logger.info("ä½¿ç”¨ä¼ ç»ŸæŸ¥è¯¢å¤„ç†é€»è¾‘")
    
    # æŸ¥è¯¢é¢„å¤„ç†
    processed_query = _preprocess_query(request.question)
    logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query}")

    # ä½¿ç”¨ä¼ ç»ŸæŸ¥è¯¢è·¯ç”±å™¨è¿›è¡Œæ™ºèƒ½è·¯ç”±
    user_ctx_dict = _build_user_context(request.user_ctx)
    route_result = legacy_query_router.route(processed_query, vector_store, user_ctx_dict)
    
    strategy = route_result['strategy']
    documents = route_result['documents']
    evaluation = route_result['evaluation']
    
    logger.info(f"ä¼ ç»Ÿè·¯ç”±ç­–ç•¥: {strategy}, æ–‡æ¡£æ•°: {len(documents)}")
    
    # æ ¹æ®ç­–ç•¥ç”Ÿæˆå›ç­”
    if strategy == 'document_based':
        # åŸºäºæ–‡æ¡£çš„å›ç­”
        answer = hr_agent.generate_response(processed_query, documents, user_ctx_dict)
    else:
        # é€šç”¨çŸ¥è¯†å›ç­”
        answer = hr_agent.generate_response(processed_query, [], user_ctx_dict)
    
    logger.info("ä¼ ç»Ÿå›ç­”ç”Ÿæˆå®Œæˆ")

    # æ„å»ºå“åº”æ•°æ®
    response_data = _build_response_data(
        answer=answer,
        vector_results=documents,
        query_analysis={'strategy': strategy},
        original_query=request.question,
        processed_query=processed_query,
        start_time=start_time,
        evaluation=evaluation
    )

    logger.info("ä¼ ç»ŸæŸ¥è¯¢å¤„ç†æˆåŠŸå®Œæˆ")
    return response_data

def _handle_timeout_error(query: str, start_time: float) -> QueryResponse:
    """å¤„ç†æŸ¥è¯¢è¶…æ—¶é”™è¯¯"""
    return QueryResponse(
        answer=f"æŸ¥è¯¢ã€Œ{query}ã€å¤„ç†è¶…æ—¶ï¼Œè¯·å°è¯•ï¼š\n\nâ€¢ ç®€åŒ–æŸ¥è¯¢å†…å®¹\nâ€¢ ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\nâ€¢ ç¨åå†è¯•\n\nå¦‚é—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
        source_data=[{
            "tool": "timeout_handler",
            "error_type": "QueryTimeout",
            "timeout_seconds": QUERY_TIMEOUT,
            "timestamp": datetime.now().isoformat()
        }],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def _preprocess_query(query: str) -> str:
    """æŸ¥è¯¢é¢„å¤„ç†å’Œä¼˜åŒ–"""
    try:
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        processed = query.strip()
        
        # æ ‡å‡†åŒ–å¸¸è§äººäº‹æœ¯è¯­
        hr_term_mapping = {
            'å·¥èµ„': 'è–ªèµ„',
            'è–ªæ°´': 'è–ªèµ„',
            'å¹´å‡': 'å¹´ä¼‘å‡',
            'ç—…å‡': 'ç—…äº‹å‡',
            'è¿Ÿåˆ°': 'è€ƒå‹¤',
            'æ—©é€€': 'è€ƒå‹¤',
            'æ–°äºº': 'æ–°å‘˜å·¥',
            'ç¦»èŒæ‰‹ç»­': 'ç¦»èŒæµç¨‹'
        }
        
        for old_term, new_term in hr_term_mapping.items():
            processed = processed.replace(old_term, new_term)
        
        # æ·»åŠ æŸ¥è¯¢æ‰©å±•ï¼ˆåŒä¹‰è¯ï¼‰
        if len(processed) < 10:  # çŸ­æŸ¥è¯¢éœ€è¦æ‰©å±•
            expansion_map = {
                'è–ªèµ„': 'è–ªèµ„ å·¥èµ„ è–ªé…¬',
                'è€ƒå‹¤': 'è€ƒå‹¤ æ‰“å¡ å‡ºå‹¤',
                'è¯·å‡': 'è¯·å‡ ä¼‘å‡ å‡æœŸ',
                'åŸ¹è®­': 'åŸ¹è®­ å­¦ä¹  å‘å±•'
            }
            
            for key, expansion in expansion_map.items():
                if key in processed:
                    processed = expansion
                    break
        
        return processed
        
    except Exception as e:
        logger.warning(f"æŸ¥è¯¢é¢„å¤„ç†å¤±è´¥: {e}")
        return query

def _execute_vector_search(query: str, query_analysis: dict, user_ctx: UserContext) -> list:
    """æ‰§è¡Œå‘é‡æœç´¢ - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        if not vector_manager:
            logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []

        # æ™ºèƒ½æœç´¢å‚æ•°è°ƒæ•´
        confidence = query_analysis.get('confidence', 0.0)
        primary_intent = query_analysis.get('primary_intent', 'general')
        
        # æ ¹æ®æ„å›¾å’Œç½®ä¿¡åº¦è°ƒæ•´æœç´¢å‚æ•°
        if confidence > 0.7:
            search_k = 8  # é«˜ç½®ä¿¡åº¦ï¼Œå¤šæ£€ç´¢
        elif confidence > 0.3:
            search_k = 5  # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæ ‡å‡†æ£€ç´¢
        else:
            search_k = 3  # ä½ç½®ä¿¡åº¦ï¼Œå°‘æ£€ç´¢

        # æ„å»ºæ™ºèƒ½è¿‡æ»¤æ¡ä»¶
        filter_metadata = _build_search_filters(primary_intent, user_ctx)

        # æ‰§è¡Œå‘é‡æœç´¢
        results = vector_manager.search_documents(
            query=query,
            k=search_k,
            filter_metadata=filter_metadata
        )
        
        if results is None:
            results = []
            
        logger.info(f"å‘é‡æœç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        return results
        
    except Exception as e:
        logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return []

def _build_search_filters(primary_intent: str, user_ctx: UserContext) -> dict:
    """æ„å»ºæœç´¢è¿‡æ»¤æ¡ä»¶"""
    filter_metadata = {}
    
    # æš‚æ—¶ç¦ç”¨åˆ†ç±»è¿‡æ»¤ï¼Œå› ä¸ºä¸Šä¼ çš„æ–‡æ¡£å¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†ç±»
    # æ ¹æ®æ„å›¾è®¾ç½®åˆ†ç±»è¿‡æ»¤
    # intent_category_map = {
    #     'policy': 'æ”¿ç­–åˆ¶åº¦',
    #     'process': 'æµç¨‹æŒ‡å—', 
    #     'benefit': 'ç¦åˆ©å¾…é‡',
    #     'attendance': 'è€ƒå‹¤ç®¡ç†',
    #     'salary': 'è–ªé…¬ç®¡ç†',
    #     'training': 'åŸ¹è®­èµ„æ–™',
    #     'onboarding': 'å…¥èŒæŒ‡å—',
    #     'offboarding': 'ç¦»èŒæŒ‡å—'
    # }
    # 
    # if primary_intent in intent_category_map:
    #     filter_metadata['category'] = intent_category_map[primary_intent]
    
    # æ ¹æ®ç”¨æˆ·è§’è‰²è®¾ç½®è®¿é—®çº§åˆ«è¿‡æ»¤
    role_access_map = {
        'employee': ['public', 'internal'],
        'hr_staff': ['public', 'internal', 'confidential'],
        'hr_manager': ['public', 'internal', 'confidential', 'restricted'],
        'hr_director': ['public', 'internal', 'confidential', 'restricted', 'secret']
    }
    
    user_role = user_ctx.user_role
    if user_role in role_access_map:
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å‘é‡æ•°æ®åº“æ”¯æŒ $in æ“ä½œç¬¦
        # å¦‚æœä¸æ”¯æŒï¼Œå¯ä»¥å»æ‰è¿™ä¸ªè¿‡æ»¤æ¡ä»¶
        pass  # æš‚æ—¶ä¸ä½¿ç”¨è®¿é—®çº§åˆ«è¿‡æ»¤ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
    
    return filter_metadata if filter_metadata else None

def _build_user_context(user_ctx: UserContext) -> dict:
    """æ„å»ºç”¨æˆ·ä¸Šä¸‹æ–‡å­—å…¸"""
    return {
        'user_role': user_ctx.user_role,
        'department': user_ctx.department,
        'user_id': user_ctx.user_id
    }

def _filter_relevant_documents_for_stream(documents, intent_analysis, retrieval_strategy, user_ctx):
    """è¿‡æ»¤æµå¼æŸ¥è¯¢çš„æ–‡æ¡£ç›¸å…³æ€§
    
    æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸æŸ¥è¯¢æ„å›¾åŒ¹é…ï¼š
    1. **è¶…çº§ç®¡ç†å‘˜è·³è¿‡è¿‡æ»¤ï¼Œå¯ä»¥è®¿é—®æ‰€æœ‰æ–‡æ¡£**
    2. å¦‚æœæ£€æµ‹åˆ°çš„éƒ¨é—¨ä¸ºnullæˆ–"é€šç”¨"ï¼Œè¯´æ˜æŸ¥è¯¢ä¸å…¬å¸ä¸šåŠ¡æ— å…³ï¼Œä¸ä½¿ç”¨æ–‡æ¡£
    3. å¦‚æœç”¨æˆ·æŸ¥è¯¢ç‰¹å®šéƒ¨é—¨çš„å†…å®¹ï¼Œæ£€æŸ¥æ–‡æ¡£æ˜¯å¦æ¥è‡ªè¯¥éƒ¨é—¨æˆ–ç›¸å…³
    4. ä½¿ç”¨æ–‡æ¡£æ ‡é¢˜å’Œéƒ¨é—¨ä¿¡æ¯è¿›è¡ŒåŒ¹é…
    """
    # ç‰¹æ®Šå¤„ç†ï¼šè¶…çº§ç®¡ç†å‘˜å¯ä»¥è®¿é—®æ‰€æœ‰æ–‡æ¡£ï¼Œè·³è¿‡è¿‡æ»¤
    if user_ctx and user_ctx.user_role == "super_admin":
        logger.info(
            f"ç”¨æˆ·æ˜¯è¶…çº§ç®¡ç†å‘˜ï¼Œè·³è¿‡æ–‡æ¡£ç›¸å…³æ€§è¿‡æ»¤ï¼Œè¿”å›æ‰€æœ‰ {len(documents)} ä¸ªæ–‡æ¡£"
        )
        return documents
    
    if not intent_analysis:
        # æ²¡æœ‰æ„å›¾åˆ†æï¼Œè¿”å›æ‰€æœ‰æ–‡æ¡£
        return documents
    
    detected_dept = intent_analysis.detected_department
    keywords = intent_analysis.keywords if hasattr(intent_analysis, 'keywords') else []
    confidence = intent_analysis.confidence if hasattr(intent_analysis, 'confidence') else 0.0
    
    # ç‰¹æ®Šæƒ…å†µ1ï¼šå¦‚æœæ£€æµ‹åˆ°çš„éƒ¨é—¨ä¸ºnullï¼Œè¯´æ˜æŸ¥è¯¢ä¸å…¬å¸ä¸šåŠ¡æ— å…³
    if not detected_dept or detected_dept == "null":
        logger.info(
            f"æ£€æµ‹åˆ°ä¸å…¬å¸ä¸šåŠ¡æ— å…³çš„æŸ¥è¯¢ï¼ˆéƒ¨é—¨: {detected_dept}ï¼‰ï¼Œä¸ä½¿ç”¨å…¬å¸æ–‡æ¡£ï¼Œåˆ‡æ¢åˆ°é€šç”¨çŸ¥è¯†å›ç­”"
        )
        return []
    
    # ç‰¹æ®Šæƒ…å†µ2ï¼šå¦‚æœæ£€æµ‹åˆ°çš„éƒ¨é—¨æ˜¯"é€šç”¨"ã€"å…¶ä»–"ã€"general"ç­‰ï¼Œè¯´æ˜æŸ¥è¯¢ä¸å…¬å¸ä¸šåŠ¡æ— å…³
    if detected_dept.lower() in ['é€šç”¨', 'å…¶ä»–', 'general', 'other', 'å…¬å…±']:
        logger.info(
            f"æ£€æµ‹åˆ°é€šç”¨æŸ¥è¯¢ï¼ˆéƒ¨é—¨: {detected_dept}ï¼‰ï¼Œä¸ä½¿ç”¨å…¬å¸æ–‡æ¡£ï¼Œåˆ‡æ¢åˆ°é€šç”¨çŸ¥è¯†å›ç­”"
        )
        return []
    
    # ç‰¹æ®Šæƒ…å†µ3ï¼šå¦‚æœç½®ä¿¡åº¦å¾ˆä½ï¼ˆ<0.3ï¼‰ï¼Œè¯´æ˜æŸ¥è¯¢æ„å›¾ä¸æ˜ç¡®ï¼Œå¯èƒ½ä¸å…¬å¸ä¸šåŠ¡æ— å…³
    if confidence < 0.3:
        logger.info(
            f"æ„å›¾ç½®ä¿¡åº¦è¿‡ä½ï¼ˆ{confidence:.2f}ï¼‰ï¼Œå¯èƒ½ä¸å…¬å¸ä¸šåŠ¡æ— å…³ï¼Œä¸ä½¿ç”¨å…¬å¸æ–‡æ¡£"
        )
        return []
    
    logger.info(
        f"è¿‡æ»¤æ–‡æ¡£ç›¸å…³æ€§: æ£€æµ‹éƒ¨é—¨={detected_dept}, "
        f"ç½®ä¿¡åº¦={confidence:.2f}, å…³é”®è¯={keywords}, æ–‡æ¡£æ•°={len(documents)}"
    )
    
    relevant_docs = []
    for doc in documents:
        # æ£€æŸ¥1ï¼šæ–‡æ¡£éƒ¨é—¨æ˜¯å¦åŒ¹é…
        if doc.department == detected_dept:
            relevant_docs.append(doc)
            logger.debug(f"âœ“ æ–‡æ¡£ç›¸å…³ï¼ˆéƒ¨é—¨åŒ¹é…ï¼‰: {doc.title} ({doc.department})")
            continue
        
        # æ£€æŸ¥2ï¼šæ–‡æ¡£æ ‡é¢˜æ˜¯å¦åŒ…å«æ£€æµ‹åˆ°çš„éƒ¨é—¨åç§°
        if detected_dept in doc.title:
            relevant_docs.append(doc)
            logger.debug(f"âœ“ æ–‡æ¡£ç›¸å…³ï¼ˆæ ‡é¢˜åŒ…å«éƒ¨é—¨ï¼‰: {doc.title}")
            continue
        
        # æ£€æŸ¥3ï¼šæ–‡æ¡£æ ‡é¢˜æ˜¯å¦åŒ…å«æŸ¥è¯¢å…³é”®è¯
        if keywords:
            title_lower = doc.title.lower()
            if any(keyword.lower() in title_lower for keyword in keywords):
                relevant_docs.append(doc)
                logger.debug(f"âœ“ æ–‡æ¡£ç›¸å…³ï¼ˆæ ‡é¢˜åŒ…å«å…³é”®è¯ï¼‰: {doc.title}")
                continue
        
        # æ–‡æ¡£ä¸ç›¸å…³
        logger.debug(
            f"âœ— æ–‡æ¡£ä¸ç›¸å…³: {doc.title} (éƒ¨é—¨: {doc.department}, "
            f"æ£€æµ‹éƒ¨é—¨: {detected_dept})"
        )
    
    logger.info(
        f"æ–‡æ¡£ç›¸å…³æ€§è¿‡æ»¤å®Œæˆ: {len(documents)} -> {len(relevant_docs)}"
    )
    
    return relevant_docs

def _handle_empty_results(query: str, start_time: float) -> QueryResponse:
    """å¤„ç†ç©ºç»“æœçš„æƒ…å†µ"""
    # æä¾›æ™ºèƒ½å»ºè®®
    suggestions = _generate_query_suggestions(query)
    
    answer = f"""æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{query}ã€ç›´æ¥ç›¸å…³çš„äººäº‹æ”¿ç­–ä¿¡æ¯ã€‚

ã€å»ºè®®å°è¯•ã€‘
{chr(10).join(f'â€¢ {suggestion}' for suggestion in suggestions)}

ã€å…¶ä»–æ–¹å¼ã€‘
â€¢ è”ç³»äººäº‹éƒ¨é—¨è·å–æœ€æ–°æ”¿ç­–ä¿¡æ¯
â€¢ æŸ¥é˜…å…¬å¸å†…éƒ¨äººäº‹ç®¡ç†ç³»ç»Ÿ
â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æŸ¥è¯¢"""

    return QueryResponse(
        answer=answer,
        source_data=[],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def _generate_query_suggestions(query: str) -> list:
    """ç”ŸæˆæŸ¥è¯¢å»ºè®®"""
    suggestions = []
    
    # åŸºäºå¸¸è§äººäº‹æŸ¥è¯¢çš„å»ºè®®
    common_queries = {
        'è–ªèµ„': ['è–ªèµ„å‘æ”¾æ—¶é—´', 'è–ªèµ„æ„æˆè¯´æ˜', 'è–ªèµ„è°ƒæ•´æ”¿ç­–'],
        'è¯·å‡': ['å¹´ä¼‘å‡ç”³è¯·æµç¨‹', 'ç—…å‡æ”¿ç­–', 'äº‹å‡è§„å®š'],
        'è€ƒå‹¤': ['è€ƒå‹¤åˆ¶åº¦', 'æ‰“å¡è¦æ±‚', 'è¿Ÿåˆ°æ—©é€€å¤„ç†'],
        'åŸ¹è®­': ['åŸ¹è®­è®¡åˆ’', 'åŸ¹è®­æŠ¥å', 'åŸ¹è®­è¯ä¹¦'],
        'å…¥èŒ': ['æ–°å‘˜å·¥å…¥èŒæµç¨‹', 'å…¥èŒææ–™æ¸…å•', 'è¯•ç”¨æœŸæ”¿ç­–'],
        'ç¦»èŒ': ['ç¦»èŒæ‰‹ç»­åŠç†', 'ç¦»èŒè¯æ˜', 'å·¥ä½œäº¤æ¥']
    }
    
    for keyword, related_queries in common_queries.items():
        if keyword in query:
            suggestions.extend(related_queries[:2])  # æ¯ä¸ªç±»åˆ«æœ€å¤š2ä¸ªå»ºè®®
            break
    
    if not suggestions:
        suggestions = ['å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯', 'æŸ¥è¯¢å…·ä½“çš„æ”¿ç­–åç§°', 'æè¿°å…·ä½“çš„ä¸šåŠ¡åœºæ™¯']
    
    return suggestions[:3]  # æœ€å¤š3ä¸ªå»ºè®®

def _build_response_data(answer: str, vector_results: list, query_analysis: dict, 
                        original_query: str, processed_query: str, start_time: float,
                        evaluation: dict = None) -> QueryResponse:
    """æ„å»ºä¼˜åŒ–çš„å“åº”æ•°æ®ï¼ŒåŒ…å«æ˜ç¡®çš„æ¥æºæ ‡è¯†"""
    strategy = query_analysis.get('strategy', 'unknown')
    
    # ç¡®å®šä¿¡æ¯æ¥æºç±»å‹
    if strategy == 'document_based' and vector_results:
        source_type = "company_documents"  # å…¬å¸ä¸Šä¼ çš„æ–‡æ¡£
        source_description = "å·²ä¸Šä¼ çš„å…¬å¸æ–‡æ¡£"
    else:
        source_type = "general_knowledge"  # AIé€šç”¨çŸ¥è¯†
        source_description = "AIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰"
    
    # æ„å»ºè¯¦ç»†çš„æ¥æºæ•°æ®
    source_data = [{
        "tool": "hr_knowledge_search",
        "source_type": source_type,  # æ–°å¢ï¼šæ˜ç¡®çš„æ¥æºç±»å‹
        "source_description": source_description,  # æ–°å¢ï¼šæ¥æºæè¿°
        "original_query": original_query,
        "processed_query": processed_query,
        "strategy": strategy,
        "evaluation": evaluation or {},
        "search_results": [
            {
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {},
                "relevance_score": doc.metadata.get('score', 0.0) if hasattr(doc, 'metadata') else 0.0,
                "document_id": doc.metadata.get('document_id', '') if hasattr(doc, 'metadata') else '',
                "title": doc.metadata.get('title', '') if hasattr(doc, 'metadata') else ''
            } for doc in vector_results
        ],
        "result_count": len(vector_results),
        "document_titles": list(set([
            doc.metadata.get('title', '') 
            for doc in vector_results 
            if hasattr(doc, 'metadata') and doc.metadata.get('title')
        ]))  # æ–°å¢ï¼šå‚è€ƒçš„æ–‡æ¡£æ ‡é¢˜åˆ—è¡¨
    }]

    # ä¼˜åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—
    if evaluation:
        confidence = evaluation.get('max_score', 0.5)
    else:
        confidence = _calculate_response_confidence(vector_results, query_analysis)

    return QueryResponse(
        answer=answer,
        source_data=source_data,
        confidence=confidence,
        processing_time=time.time() - start_time
    )



def _calculate_response_confidence(vector_results: list, query_analysis: dict) -> float:
    """è®¡ç®—å“åº”ç½®ä¿¡åº¦"""
    try:
        # åŸºç¡€ç½®ä¿¡åº¦ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡
        result_count = len(vector_results)
        base_confidence = min(result_count / 5.0, 1.0)
        
        # æ„å›¾åˆ†æç½®ä¿¡åº¦
        intent_confidence = query_analysis.get('confidence', 0.0)
        
        # æ–‡æ¡£ç›¸å…³æ€§ç½®ä¿¡åº¦
        if vector_results:
            scores = [getattr(doc, 'score', 0.0) for doc in vector_results]
            avg_score = sum(scores) / len(scores) if scores else 0.0
            relevance_confidence = min(avg_score, 1.0)
        else:
            relevance_confidence = 0.0
        
        # ç»¼åˆç½®ä¿¡åº¦è®¡ç®—
        confidence = (base_confidence * 0.4 + intent_confidence * 0.3 + relevance_confidence * 0.3)
        
        return round(confidence, 3)
        
    except Exception as e:
        logger.warning(f"ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦

def _handle_query_error(error: Exception, start_time: float) -> QueryResponse:
    """å¤„ç†æŸ¥è¯¢é”™è¯¯"""
    error_msg = str(error)
    
    # æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„ç”¨æˆ·å‹å¥½æ¶ˆæ¯
    if "timeout" in error_msg.lower():
        user_message = "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•æˆ–ç®€åŒ–æŸ¥è¯¢å†…å®¹ã€‚"
    elif "connection" in error_msg.lower():
        user_message = "ç³»ç»Ÿè¿æ¥å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•ã€‚"
    elif "memory" in error_msg.lower():
        user_message = "ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œè¯·ç¨åå†è¯•ã€‚"
    else:
        user_message = "ç³»ç»Ÿå¤„ç†å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
    
    return QueryResponse(
        answer=user_message,
        source_data=[{
            "tool": "error_handler",
            "error_type": type(error).__name__,
            "error_message": error_msg,
            "timestamp": datetime.now().isoformat()
        }],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def service_upload_document(file, title: str, category: str, access_level: str, user_ctx: str) -> DocumentUploadResponse:
    """å¤„ç†æ–‡æ¡£ä¸Šä¼ è¯·æ±‚ - ä¼˜åŒ–ç‰ˆæœ¬"""
    import shutil
    from pathlib import Path
    from config import UPLOAD_DIR
    
    try:
        logger.info(f"æ”¶åˆ°æ–‡æ¡£ä¸Šä¼ : {file.filename} (æ ‡é¢˜: {title})")

        if vector_manager is None:
            logger.error('å‘é‡å­˜å‚¨ç®¡ç†å™¨æœªåˆå§‹åŒ–')
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        user_context = json.loads(user_ctx)
        
        # æ·»åŠ æ—¥å¿—ï¼šæŸ¥çœ‹å‰ç«¯ä¼ é€’çš„user_contextå†…å®¹
        logger.info(f"ä¸Šä¼ æ–‡æ¡£ - å‰ç«¯ä¼ é€’çš„user_context: {user_context}")
        logger.info(f"ä¸Šä¼ æ–‡æ¡£ - ç›®æ ‡éƒ¨é—¨: {user_context.get('department', 'HR')}")

        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
        if vector_manager.check_duplicate_title(title):
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£æ ‡é¢˜ '{title}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜")

        # 1. ä¿å­˜æ–‡ä»¶åˆ°æœ¬åœ°
        upload_dir = Path(UPLOAD_DIR)
        upload_dir.mkdir(parents=True, exist_ok=True)  # å¦‚æœç›®å½•ä¸å­˜åœ¨åˆ™åˆ›å»º
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åï¼ˆæ—¶é—´æˆ³ + åŸæ–‡ä»¶åï¼‰
        timestamp = int(time.time())
        file_extension = Path(file.filename).suffix
        safe_filename = f"{timestamp}_{file.filename}"
        file_path = upload_dir / safe_filename
        
        # ä¿å­˜æ–‡ä»¶
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
        
        logger.info(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
        
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆï¼Œä»¥ä¾¿åç»­å¤„ç†
        file.file.seek(0)

        # 2. å¤„ç†æ–‡æ¡£å¹¶å­˜å…¥å‘é‡æ•°æ®åº“
        try:
            document_chunks = process_upload_file(file)
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            # å¦‚æœå¤„ç†å¤±è´¥ï¼Œåˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id_base = f"hr_doc_{timestamp}"
        document_id = doc_id_base

        # ä¼˜åŒ–çš„å…ƒæ•°æ®ç»“æ„ï¼Œé€‚é…äººäº‹åœºæ™¯
        metadatas = [{
            "document_id": document_id,
            "doc_type": "uploaded",
            "access_level": access_level,
            "title": title,
            "category": category,
            "source_file": file.filename,
            "file_path": str(file_path),  # ä¿å­˜æ–‡ä»¶è·¯å¾„
            "department": user_context.get("department", "HR"),
            "uploader": user_context.get("user_role", "hr_staff"),
            "upload_time": datetime.now().isoformat(),
            "chunk_index": i,
            "total_chunks": len(document_chunks)
        } for i, _ in enumerate(document_chunks)]

        chunk_ids = [f"{doc_id_base}_chunk_{i}" for i in range(len(document_chunks))]

        # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡å­˜å‚¨ç®¡ç†å™¨
        success = vector_manager.add_document(
            texts=document_chunks,
            metadatas=metadatas,
            ids=chunk_ids
        )

        if not success:
            # å¦‚æœå‘é‡æ•°æ®åº“æ·»åŠ å¤±è´¥ï¼Œåˆ é™¤å·²ä¿å­˜çš„æ–‡ä»¶
            if file_path.exists():
                file_path.unlink()
            raise HTTPException(status_code=500, detail="æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“å¤±è´¥")

        logger.info(f"æˆåŠŸæ·»åŠ  {len(document_chunks)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")

        return DocumentUploadResponse(
            status="success",
            document_id=document_id,
            filename=file.filename,
            chunks=len(document_chunks),
            first_chunk=document_chunks[0][:100] + "..." if document_chunks else "",
            message=f"æ–‡æ¡£ '{title}' ä¸Šä¼ æˆåŠŸï¼Œå·²åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªæ–‡æ¡£å—"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")

def service_delete_document(document_id: str) -> dict:
    """åˆ é™¤æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        success = vector_manager.delete_document(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"æ–‡æ¡£ {document_id} åˆ é™¤æˆåŠŸ"
            }
        else:
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_update_document(document_id: str, file, title: str, category: str, access_level: str, user_ctx: str) -> DocumentUploadResponse:
    """æ›´æ–°æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        user_context = json.loads(user_ctx)

        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä¸å…¶ä»–æ–‡æ¡£é‡å¤ï¼ˆæ’é™¤å½“å‰æ–‡æ¡£ï¼‰
        if vector_manager.check_duplicate_title(title, exclude_doc_id=document_id):
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£æ ‡é¢˜ '{title}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜")

        try:
            document_chunks = process_upload_file(file)
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

        # ä½¿ç”¨åŸæœ‰çš„æ–‡æ¡£ID
        metadatas = [{
            "document_id": document_id,
            "doc_type": "uploaded",
            "access_level": access_level,
            "title": title,
            "category": category,
            "source_file": file.filename,
            "department": user_context.get("department", "HR"),
            "uploader": user_context.get("user_role", "hr_staff"),
            "upload_time": datetime.now().isoformat(),
            "chunk_index": i,
            "total_chunks": len(document_chunks)
        } for i, _ in enumerate(document_chunks)]

        chunk_ids = [f"{document_id}_chunk_{i}" for i in range(len(document_chunks))]

        success = vector_manager.update_document(
            document_id=document_id,
            texts=document_chunks,
            metadatas=metadatas,
            ids=chunk_ids
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ–‡æ¡£æ›´æ–°å¤±è´¥")

        return DocumentUploadResponse(
            status="success",
            document_id=document_id,
            filename=file.filename,
            chunks=len(document_chunks),
            first_chunk=document_chunks[0][:100] + "..." if document_chunks else "",
            message=f"æ–‡æ¡£ '{title}' æ›´æ–°æˆåŠŸï¼Œå·²åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªæ–‡æ¡£å—"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_list_documents(limit: Optional[int] = None) -> List[DocumentInfo]:
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        documents = vector_manager.list_documents(limit=limit)
        
        # è½¬æ¢ä¸ºDocumentInfoæ ¼å¼
        doc_infos = []
        for doc in documents:
            # å®‰å…¨åœ°è½¬æ¢æšä¸¾ç±»å‹
            category_str = doc.get('category', 'å…¶ä»–')
            access_level_str = doc.get('access_level', 'å…¨å‘˜')
            
            # æŸ¥æ‰¾åŒ¹é…çš„æšä¸¾å€¼
            category = DocumentCategory.OTHER
            for cat in DocumentCategory:
                if cat.value == category_str:
                    category = cat
                    break
            
            access_level = AccessLevel.PUBLIC
            for level in AccessLevel:
                if level.value == access_level_str:
                    access_level = level
                    break
            
            doc_info = DocumentInfo(
                id=doc.get('document_id', ''),
                title=doc.get('title', ''),
                category=category,
                access_level=access_level,
                filename=doc.get('source_file', ''),
                upload_time=datetime.fromisoformat(doc.get('upload_time', datetime.now().isoformat())),
                uploader=doc.get('uploader', ''),
                chunks_count=doc.get('total_chunks', 0),
                department=doc.get('department', '')
            )
            doc_infos.append(doc_info)
        
        return doc_infos
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")

def service_search_documents(query: str, category: Optional[str] = None, access_level: Optional[str] = None, k: int = 5) -> List[dict]:
    """æœç´¢æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_metadata = {}
        if category:
            filter_metadata['category'] = category
        if access_level:
            filter_metadata['access_level'] = access_level

        results = vector_manager.search_documents(
            query=query,
            k=k,
            filter_metadata=filter_metadata if filter_metadata else None
        )

        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = []
        for doc in results:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {},
                "score": getattr(doc, 'score', 0.0)
            })

        return formatted_results
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_health_check() -> HealthResponse:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    # æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
    vector_db_status = "active" if vector_store else "inactive"
    
    # å¦‚æœå…³é”®ç»„ä»¶ä¸å¯ç”¨ï¼Œç³»ç»ŸçŠ¶æ€ä¸ºè­¦å‘Š
    system_status = "ok" if vector_store else "warning"
    
    detail = {
        "version": "2.0",
        "mode": "api" if vector_store else "simplified",
        "components": {
            "vector_db": vector_db_status,
            "llm_integration": "alibaba_cloud",
            "api_server": "active",
            "query_router": "active" if legacy_query_router else "inactive"
        }
    }
    
    # å¦‚æœæ˜¯ç®€åŒ–æ¨¡å¼ï¼Œæ·»åŠ è¯´æ˜
    if not vector_store:
        detail["notice"] = "ç³»ç»Ÿè¿è¡Œåœ¨ç®€åŒ–æ¨¡å¼ï¼Œå‘é‡æœç´¢åŠŸèƒ½ä¸å¯ç”¨ã€‚"
    
    return HealthResponse(
        status=system_status,
        service="äººäº‹çŸ¥è¯†åº“ç³»ç»Ÿ",
        detail=detail
    )

def service_vector_status() -> VectorStoreStatus:
    """è·å–å‘é‡å­˜å‚¨çŠ¶æ€ - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        if vector_manager:
            stats = vector_manager.get_collection_stats()
            
            return VectorStoreStatus(
                status="active",
                documents=stats.get('total_documents', 0),
                collection_name=stats.get('collection_name', 'hr_knowledge'),
                last_updated=datetime.fromisoformat(stats.get('last_updated', datetime.now().isoformat()))
            )
        else:
            return VectorStoreStatus(
                status="not_initialized",
                documents=0,
                collection_name="hr_knowledge"
            )
    except Exception as e:
        logger.error(f"å‘é‡æ•°æ®åº“çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
        return VectorStoreStatus(
            status="error",
            documents=0,
            collection_name="hr_knowledge"
        )

def service_get_collection_stats() -> dict:
    """è·å–å‘é‡å­˜å‚¨è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        stats = vector_manager.get_collection_stats()
        return {
            "status": "success",
            "data": stats
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

async def service_query_knowledge_stream(request: QueryRequest) -> AsyncGenerator[dict, None]:
    """å¤„ç†äººäº‹çŸ¥è¯†æŸ¥è¯¢è¯·æ±‚ - æµå¼ç‰ˆæœ¬ï¼ˆä½¿ç”¨æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨ + å¯¹è¯å†å²ï¼‰"""
    start_time = time.time()
    logger.info(f"æ”¶åˆ°æµå¼æŸ¥è¯¢: {request.question}, session_id={request.session_id}")

    try:
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„æ™ºèƒ½è·¯ç”±å™¨
        if intelligent_router:
            logger.info("ä½¿ç”¨æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±å™¨å¤„ç†æµå¼æŸ¥è¯¢")
            
            # dev-mixæ–°å¢ï¼šè·å–å¯¹è¯å†å²
            history_manager = get_history_manager()
            chat_history = None
            if request.session_id:
                chat_history = history_manager.get_history(request.session_id)
                logger.info(f"ä¼šè¯ {request.session_id} å†å²æ¶ˆæ¯æ•°: {len(chat_history.messages)}")
            
            # 1. å‘é€æŸ¥è¯¢é¢„å¤„ç†çŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'preprocessing',
                'message': 'æ­£åœ¨é¢„å¤„ç†æŸ¥è¯¢...',
                'progress': 10
            }
            
            # 2. å‘é€æ™ºèƒ½è·¯ç”±åˆ†æçŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'intelligent_routing',
                'message': 'æ­£åœ¨è¿›è¡Œæ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±åˆ†æ...',
                'progress': 30
            }

            # ä½¿ç”¨æ™ºèƒ½è·¯ç”±å™¨å¤„ç†æŸ¥è¯¢
            query_result = intelligent_router.route_query(
                query=request.question,
                user_id=request.user_ctx.user_id,
                session_id=request.session_id
            )
            
            logger.info(f"æ™ºèƒ½è·¯ç”±å®Œæˆ: ç­–ç•¥={query_result.source_type}, æ–‡æ¡£æ•°={len(query_result.documents)}")
            
            # 3. å‘é€LLMç”Ÿæˆå¼€å§‹çŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'llm_generation',
                'message': f'æ­£åœ¨ç”Ÿæˆå›ç­”ï¼ˆç­–ç•¥: {query_result.source_type}ï¼‰...',
                'progress': 50
            }

            # 4. æµå¼ç”ŸæˆLLMå›ç­”ï¼ˆåŸºäºæ™ºèƒ½è·¯ç”±ç»“æœï¼‰
            logger.info("å¼€å§‹åŸºäºæ™ºèƒ½è·¯ç”±ç»“æœçš„æµå¼LLMå›ç­”ç”Ÿæˆ...")
            
            # è¿‡æ»¤æ–‡æ¡£ç›¸å…³æ€§
            docs_to_use = []
            if query_result.source_type == "document_based" and query_result.documents:
                # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦ä¸æŸ¥è¯¢æ„å›¾ç›¸å…³
                intent_analysis = query_result.intent_analysis
                retrieval_strategy = query_result.retrieval_strategy
                
                relevant_docs = _filter_relevant_documents_for_stream(
                    query_result.documents,
                    intent_analysis,
                    retrieval_strategy,
                    request.user_ctx
                )
                
                if not relevant_docs:
                    logger.warning(
                        f"æ£€ç´¢åˆ°{len(query_result.documents)}ä¸ªæ–‡æ¡£ï¼Œä½†éƒ½ä¸æŸ¥è¯¢æ„å›¾ä¸ç›¸å…³ã€‚"
                        f"æ£€æµ‹éƒ¨é—¨: {intent_analysis.detected_department}"
                    )
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯æƒé™é—®é¢˜
                    if (intent_analysis.detected_department and 
                        retrieval_strategy and 
                        not retrieval_strategy.has_permission):
                        # ç”¨æˆ·æŸ¥è¯¢çš„æ˜¯æ— æƒé™éƒ¨é—¨çš„å†…å®¹
                        permission_denied_answer = f"""ã€æƒé™æç¤ºã€‘

æ‚¨æŸ¥è¯¢çš„å†…å®¹æ¶‰åŠã€Œ{intent_analysis.detected_department}ã€éƒ¨é—¨ï¼Œä½†æ‚¨å½“å‰æ— æƒè®¿é—®è¯¥éƒ¨é—¨çš„æ–‡æ¡£ã€‚

ã€æ‚¨çš„æƒé™ã€‘
- å½“å‰éƒ¨é—¨ï¼š{request.user_ctx.department}

ã€å»ºè®®æ“ä½œã€‘
1. å¦‚éœ€æŸ¥çœ‹{intent_analysis.detected_department}éƒ¨é—¨çš„æ–‡æ¡£ï¼Œè¯·è”ç³»{intent_analysis.detected_department}éƒ¨é—¨è´Ÿè´£äººç”³è¯·æƒé™
2. æ‚¨å¯ä»¥åœ¨å…¬å…±æ–‡ä»¶å¤¹ä¸­æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯
3. è”ç³»ç³»ç»Ÿç®¡ç†å‘˜äº†è§£æƒé™ç”³è¯·æµç¨‹

ã€æ¸©é¦¨æç¤ºã€‘
å…¬å¸æ–‡æ¡£æŒ‰éƒ¨é—¨è¿›è¡Œæƒé™ç®¡ç†ï¼Œä»¥ä¿æŠ¤æ•æ„Ÿä¿¡æ¯ã€‚å¦‚æœ‰ä¸šåŠ¡éœ€è¦ï¼Œè¯·é€šè¿‡æ­£è§„æµç¨‹ç”³è¯·è®¿é—®æƒé™ã€‚

âš ï¸ å…è´£å£°æ˜ï¼šä»¥ä¸Šå›ç­”ç”±AIåŸºäºæ–‡æ¡£å†…å®¹åˆ†æç”Ÿæˆï¼Œä»…ä¾›å‚è€ƒã€‚å¦‚æœ‰ç–‘é—®æˆ–éœ€è¦å‡†ç¡®ä¿¡æ¯ï¼Œè¯·ä¸å…¬å¸ç›¸å…³éƒ¨é—¨è´Ÿè´£äººç¡®è®¤ã€‚"""
                        
                        yield {
                            'type': 'content',
                            'content': permission_denied_answer,
                            'is_complete': True,
                            'progress': 90
                        }
                        
                        # å‘é€å®ŒæˆçŠ¶æ€
                        processing_time = time.time() - start_time
                        yield {
                            'type': 'complete',
                            'message': 'æƒé™æ£€æŸ¥å®Œæˆ',
                            'progress': 100,
                            'full_answer': permission_denied_answer,
                            'confidence': 1.0,
                            'processing_time': processing_time,
                            'source_count': 0,
                            'strategy': 'permission_denied',
                            'user_department': request.user_ctx.department,
                            'intent_analysis': {
                                'primary_intent': intent_analysis.primary_intent,
                                'confidence': intent_analysis.confidence,
                                'detected_department': intent_analysis.detected_department
                            }
                        }
                        
                        logger.info("æƒé™æ‹’ç»å›ç­”å·²å‘é€")
                        return
                    else:
                        # å…¶ä»–æƒ…å†µï¼šæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œä½¿ç”¨é€šç”¨çŸ¥è¯†å›ç­”
                        logger.info("æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œåˆ‡æ¢åˆ°é€šç”¨çŸ¥è¯†å›ç­”")
                        docs_to_use = []
                else:
                    # å°†ç›¸å…³æ–‡æ¡£è½¬æ¢ä¸ºå…¼å®¹æ ¼å¼
                    for doc in relevant_docs:
                        class SimpleDoc:
                            def __init__(self, content, metadata):
                                self.page_content = content
                                self.metadata = metadata
                        
                        docs_to_use.append(SimpleDoc(
                            content=doc.content,
                            metadata={
                                'title': doc.title,
                                'department': doc.department,
                                'score': doc.score,
                                'document_id': doc.document_id
                            }
                        ))
                    
                    logger.info(f"å‡†å¤‡ä½¿ç”¨ {len(docs_to_use)} ä¸ªç›¸å…³æ–‡æ¡£è¿›è¡Œæµå¼ç”Ÿæˆ")
            else:
                logger.info(f"æ— æ–‡æ¡£ï¼Œä½¿ç”¨é€šç”¨çŸ¥è¯†å›ç­”ï¼ˆç­–ç•¥: {query_result.source_type}ï¼‰")
            
            user_ctx_dict = _build_user_context(request.user_ctx)
            
            # æµå¼ç”Ÿæˆç­”æ¡ˆ
            full_answer = ""
            chunk_count = 0
            async for chunk in _generate_streaming_response(docs_to_use, request.question, user_ctx_dict, chat_history):
                full_answer += chunk
                chunk_count += 1
                logger.info(f"[Stream] Chunk #{chunk_count}, length: {len(chunk)}, total: {len(full_answer)}")
                yield {
                    'type': 'content',
                    'content': chunk,
                    'is_complete': False,
                    'progress': min(90, 50 + (len(full_answer) / 10))
                }
            
            logger.info("æ™ºèƒ½è·¯ç”±æµå¼LLMå›ç­”ç”Ÿæˆå®Œæˆ")
            
            # dev-mixæ–°å¢ï¼šæ›´æ–°å¯¹è¯å†å²
            if request.session_id and chat_history:
                history_manager.add_user_message(request.session_id, request.question)
                history_manager.add_ai_message(request.session_id, full_answer)
                logger.info(f"å·²æ›´æ–°ä¼šè¯ {request.session_id} çš„å¯¹è¯å†å²")

            # 5. å‘é€å®ŒæˆçŠ¶æ€
            processing_time = time.time() - start_time
            
            yield {
                'type': 'complete',
                'message': 'æ™ºèƒ½å¤šéƒ¨é—¨è·¯ç”±æŸ¥è¯¢å®Œæˆ',
                'progress': 100,
                'full_answer': full_answer,
                'confidence': query_result.confidence,
                'processing_time': processing_time,
                'source_count': len(query_result.documents),
                'strategy': query_result.source_type,
                'user_department': query_result.user_context.department,
                'intent_analysis': {
                    'primary_intent': query_result.intent_analysis.primary_intent,
                    'confidence': query_result.intent_analysis.confidence,
                    'detected_department': query_result.intent_analysis.detected_department
                }
            }

            logger.info("æ™ºèƒ½å¤šéƒ¨é—¨æµå¼æŸ¥è¯¢æˆåŠŸå®Œæˆ")
            return
        
        # é™çº§åˆ°ä¼ ç»Ÿè·¯ç”±å™¨å¤„ç†æµå¼æŸ¥è¯¢
        elif legacy_query_router and vector_store:
            logger.warning("æ™ºèƒ½è·¯ç”±å™¨ä¸å¯ç”¨ï¼Œé™çº§åˆ°ä¼ ç»Ÿæµå¼è·¯ç”±å™¨")
            
            # dev-mixæ–°å¢ï¼šè·å–å¯¹è¯å†å²
            history_manager = get_history_manager()
            chat_history = None
            if request.session_id:
                chat_history = history_manager.get_history(request.session_id)
                logger.info(f"ä¼šè¯ {request.session_id} å†å²æ¶ˆæ¯æ•°: {len(chat_history.messages)}")
            
            # 1. å‘é€æŸ¥è¯¢é¢„å¤„ç†çŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'preprocessing',
                'message': 'æ­£åœ¨é¢„å¤„ç†æŸ¥è¯¢...',
                'progress': 10
            }
            
            # æŸ¥è¯¢é¢„å¤„ç†
            processed_query = _preprocess_query(request.question)
            logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query}")
            
            # 2. å‘é€è·¯ç”±åˆ†æçŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'routing',
                'message': 'æ­£åœ¨åˆ†ææŸ¥è¯¢å¹¶è·¯ç”±...',
                'progress': 30
            }

            # ä½¿ç”¨ä¼ ç»ŸæŸ¥è¯¢è·¯ç”±å™¨
            user_ctx_dict = _build_user_context(request.user_ctx)
            route_result = legacy_query_router.route(processed_query, vector_store, user_ctx_dict)
            
            strategy = route_result['strategy']
            documents = route_result['documents']
            evaluation = route_result['evaluation']
            
            logger.info(f"ä¼ ç»Ÿè·¯ç”±ç­–ç•¥: {strategy}, æ–‡æ¡£æ•°: {len(documents)}")
            
            # 3. å‘é€LLMç”Ÿæˆå¼€å§‹çŠ¶æ€
            yield {
                'type': 'status',
                'stage': 'llm_generation',
                'message': f'æ­£åœ¨ç”Ÿæˆå›ç­”ï¼ˆç­–ç•¥: {strategy}ï¼‰...',
                'progress': 50
            }

            # 4. æµå¼ç”ŸæˆLLMå›ç­”ï¼ˆdev-mixï¼šä¼ å…¥å¯¹è¯å†å²ï¼‰
            logger.info("å¼€å§‹ä¼ ç»Ÿæµå¼LLMå›ç­”ç”Ÿæˆ...")
            
            # æ ¹æ®ç­–ç•¥å†³å®šæ˜¯å¦ä¼ é€’æ–‡æ¡£
            docs_to_use = documents if strategy == 'document_based' else []
            logger.info(f"ä¼ é€’ç»™LLMçš„æ–‡æ¡£æ•°: {len(docs_to_use)} (ç­–ç•¥: {strategy})")
            
            full_answer = ""
            chunk_count = 0
            async for chunk in _generate_streaming_response(docs_to_use, processed_query, user_ctx_dict, chat_history):
                full_answer += chunk
                chunk_count += 1
                logger.info(f"[Stream] Chunk #{chunk_count}, length: {len(chunk)}, total: {len(full_answer)}")
                yield {
                    'type': 'content',
                    'content': chunk,
                    'is_complete': False,
                    'progress': min(90, 50 + (len(full_answer) / 10))
                }
            
            logger.info("ä¼ ç»Ÿæµå¼LLMå›ç­”ç”Ÿæˆå®Œæˆ")
            
            # dev-mixæ–°å¢ï¼šæ›´æ–°å¯¹è¯å†å²
            if request.session_id and chat_history:
                history_manager.add_user_message(request.session_id, request.question)
                history_manager.add_ai_message(request.session_id, full_answer)
                logger.info(f"å·²æ›´æ–°ä¼šè¯ {request.session_id} çš„å¯¹è¯å†å²")

            # 5. å‘é€å®ŒæˆçŠ¶æ€
            processing_time = time.time() - start_time
            confidence = evaluation.get('max_score', 0.5)
            
            yield {
                'type': 'complete',
                'message': 'ä¼ ç»Ÿè·¯ç”±æŸ¥è¯¢å®Œæˆ',
                'progress': 100,
                'full_answer': full_answer,
                'confidence': confidence,
                'processing_time': processing_time,
                'source_count': len(documents),
                'strategy': strategy
            }

            logger.info("ä¼ ç»Ÿæµå¼æŸ¥è¯¢æˆåŠŸå®Œæˆ")
        
        else:
            yield {
                'type': 'error',
                'message': 'æŸ¥è¯¢ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–',
                'error_type': 'SystemError'
            }

    except Exception as e:
        logger.error(f"æµå¼æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
        yield {
            'type': 'error',
            'message': f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
            'error_type': type(e).__name__
        }

async def _generate_streaming_response(vector_results: List, query: str, user_ctx: Dict, chat_history=None) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼LLMå“åº” - ä½¿ç”¨é˜¿é‡Œäº‘APIçš„æµå¼æ¥å£ï¼ˆdev-mixï¼šæ”¯æŒå¯¹è¯å†å²ï¼‰"""
    try:
        agent = get_hr_agent()
        if not agent.llm:
            yield "æŠ±æ­‰ï¼ŒAIåŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
            return
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
        if vector_results:
            # åŸºäºæ–‡æ¡£çš„å›ç­” - ä¸åœ¨å¼€å¤´æ·»åŠ æ¥æºæ ‡è¯†
            # å› ä¸ºå›ç­”ä¸­çš„ã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†å·²ç»è¯´æ˜äº†æ¥æº
            context_docs = agent._format_context_documents(vector_results)
            
            # dev-mixæ–°å¢ï¼šæ„å»ºåŒ…å«å†å²çš„æç¤ºè¯
            if chat_history and len(chat_history.messages) > 0:
                prompt = agent._build_enhanced_prompt_with_history(
                    query, context_docs, user_ctx, chat_history.messages
                )
                logger.info(f"ä½¿ç”¨å¯¹è¯å†å²æ„å»ºæç¤ºè¯ï¼Œå†å²æ¶ˆæ¯æ•°: {len(chat_history.messages)}")
            else:
                prompt = agent._build_enhanced_prompt(query, context_docs, user_ctx)
        else:
            # é€šç”¨çŸ¥è¯†å›ç­” - å…ˆè¾“å‡ºæ¥æºæ ‡è¯†
            yield "ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**\n\n"
            
            # dev-mixæ–°å¢ï¼šé€šç”¨çŸ¥è¯†å›ç­”ä¹Ÿæ”¯æŒå†å²
            if chat_history and len(chat_history.messages) > 0:
                history_text = "\n".join([
                    f"{'ç”¨æˆ·' if i % 2 == 0 else 'AI'}: {msg.content}"
                    for i, msg in enumerate(chat_history.messages[-4:])  # æœ€è¿‘2è½®å¯¹è¯
                ])
                
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ã€‚

å¯¹è¯å†å²ï¼š
{history_text}

å½“å‰é—®é¢˜ï¼š{query}

è¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœè¿™ä¸ªé—®é¢˜ä¸ä¼ä¸šç®¡ç†ç›¸å…³ï¼Œè¯·æä¾›ä¸“ä¸šçš„å»ºè®®ã€‚

æ³¨æ„ï¼š
1. ä¸è¦åœ¨å›ç­”ä¸­é‡å¤æ·»åŠ "ä¿¡æ¯æ¥æº"æ ‡è¯†ï¼Œå› ä¸ºå·²ç»åœ¨å‰é¢æ·»åŠ è¿‡äº†
2. ä¸è¦æ·»åŠ å…è´£å£°æ˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ """
            else:
                prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åº“åŠ©æ‰‹ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœè¿™ä¸ªé—®é¢˜ä¸ä¼ä¸šç®¡ç†ç›¸å…³ï¼Œè¯·æä¾›ä¸“ä¸šçš„å»ºè®®ã€‚

æ³¨æ„ï¼š
1. ä¸è¦åœ¨å›ç­”ä¸­é‡å¤æ·»åŠ "ä¿¡æ¯æ¥æº"æ ‡è¯†ï¼Œå› ä¸ºå·²ç»åœ¨å‰é¢æ·»åŠ è¿‡äº†
2. ä¸è¦æ·»åŠ å…è´£å£°æ˜ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æ·»åŠ """
        
        # ä½¿ç”¨é˜¿é‡Œäº‘APIçš„æµå¼ç”Ÿæˆ
        try:
            logger.info("å¼€å§‹ä½¿ç”¨æµå¼APIç”Ÿæˆå›ç­”...")
            
            # ä½¿ç”¨ astream è¿›è¡Œå¼‚æ­¥æµå¼ç”Ÿæˆ
            async for chunk in agent.llm.astream(prompt):
                if chunk.content:
                    # è¿‡æ»¤æ‰LLMå¯èƒ½é‡å¤æ·»åŠ çš„æ¥æºæ ‡è¯†
                    content = chunk.content
                    if "ä¿¡æ¯æ¥æº" not in content or content.index("ä¿¡æ¯æ¥æº") > 10:
                        # ç›´æ¥è¾“å‡ºæ¯ä¸ª chunkï¼Œå®ç°çœŸæ­£çš„æµå¼æ•ˆæœ
                        yield content
                    
        except Exception as e:
            error_msg = str(e)
            logger.error(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹å®¡æ ¸é”™è¯¯
            if "inappropriate content" in error_msg.lower() or "inappropriate-content" in error_msg.lower():
                yield f"\n\nâš ï¸ æŠ±æ­‰ï¼Œç”±äºå†…å®¹å®‰å…¨å®¡æ ¸é™åˆ¶ï¼Œæ— æ³•å®Œæ•´ç”Ÿæˆå›ç­”ã€‚è¿™å¯èƒ½æ˜¯å› ä¸ºæŸäº›è¯æ±‡è§¦å‘äº†å®‰å…¨æœºåˆ¶ã€‚\n\nå»ºè®®ï¼š\nâ€¢ å°è¯•æ¢ä¸€ç§æ–¹å¼æé—®\nâ€¢ ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\nâ€¢ å¦‚éœ€å¸®åŠ©ï¼Œè¯·è”ç³»ç³»ç»Ÿç®¡ç†å‘˜"
            else:
                yield f"\n\næŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é—®é¢˜: {error_msg}"
            
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}"