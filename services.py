import time
import json
import asyncio
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FutureTimeoutError
from fastapi import HTTPException
from logging_setup import logger
from knowledge_base import init_chroma, process_upload_file
from knowledge_base import get_vector_manager
from schemas import (
    QueryRequest, QueryResponse, DocumentUploadResponse, 
    HealthResponse, VectorStoreStatus, UserContext, DocumentInfo,
    DocumentCategory, AccessLevel
)
from llm_agent import integrate_results, analyze_query, get_hr_agent
from typing import List, Optional, AsyncGenerator, Dict
from config import QUERY_TIMEOUT, MAX_QUERY_RESULTS

# åˆå§‹åŒ–å‘é‡åº“ï¼ˆchromaï¼‰
try:
    # è®¾ç½®ç¯å¢ƒå˜é‡è§£å†³protobufç‰ˆæœ¬å†²çªå’Œç¦ç”¨é¥æµ‹
    import os
    os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'
    os.environ['ANONYMIZED_TELEMETRY'] = 'False'
    os.environ['CHROMA_TELEMETRY'] = 'False'
    
    from knowledge_base import init_chroma, get_vector_manager
    from query_router import create_query_router
    
    vector_store = init_chroma()
    vector_manager = get_vector_manager()
    query_router = create_query_router()
    
    # åˆå§‹åŒ–å“åº”ç”Ÿæˆå™¨
    hr_agent = get_hr_agent()
    
    if vector_store:
        logger.info('ChromaDBå‘é‡åº“åˆå§‹åŒ–æˆåŠŸï¼ˆä½¿ç”¨LangChain + é˜¿é‡Œäº‘APIï¼‰')
    else:
        logger.warning('ChromaDBå‘é‡åº“åˆå§‹åŒ–å¤±è´¥ï¼Œç³»ç»Ÿå°†ä»¥ç®€åŒ–æ¨¡å¼è¿è¡Œ')
        
except Exception as e:
    logger.error(f'å‘é‡åº“åˆå§‹åŒ–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}', exc_info=True)
    vector_store = None
    vector_manager = None
    query_router = None
    
    # ç®€åŒ–æ¨¡å¼ä¸‹çš„å“åº”ç”Ÿæˆå™¨
    hr_agent = get_hr_agent()

def service_query_knowledge(request: QueryRequest) -> QueryResponse:
    """å¤„ç†äººäº‹çŸ¥è¯†æŸ¥è¯¢è¯·æ±‚ - ä½¿ç”¨æ™ºèƒ½è·¯ç”±"""
    start_time = time.time()
    logger.info(f"æ”¶åˆ°äººäº‹çŸ¥è¯†æŸ¥è¯¢: {request.question}")

    try:
        if not query_router or not vector_store:
            raise HTTPException(status_code=500, detail="ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–")
        
        # æŸ¥è¯¢é¢„å¤„ç†
        processed_query = _preprocess_query(request.question)
        logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query}")

        # ä½¿ç”¨æŸ¥è¯¢è·¯ç”±å™¨è¿›è¡Œæ™ºèƒ½è·¯ç”±
        user_ctx_dict = _build_user_context(request.user_ctx)
        route_result = query_router.route(processed_query, vector_store, user_ctx_dict)
        
        strategy = route_result['strategy']
        documents = route_result['documents']
        evaluation = route_result['evaluation']
        
        logger.info(f"è·¯ç”±ç­–ç•¥: {strategy}, æ–‡æ¡£æ•°: {len(documents)}")
        
        # æ ¹æ®ç­–ç•¥ç”Ÿæˆå›ç­”
        if strategy == 'document_based':
            # åŸºäºæ–‡æ¡£çš„å›ç­”
            answer = hr_agent.generate_response(processed_query, documents, user_ctx_dict)
        else:
            # é€šç”¨çŸ¥è¯†å›ç­”
            answer = hr_agent.generate_response(processed_query, [], user_ctx_dict)
        
        logger.info("å›ç­”ç”Ÿæˆå®Œæˆ")

        # æ„å»ºå“åº”æ•°æ®
        response_data = _build_response_data(
            answer=answer,
            vector_results=documents,
            query_analysis={'strategy': strategy},
            original_query=request.question,
            processed_query=processed_query,
            start_time=start_time,
            evaluation=evaluation
        )

        logger.info("äººäº‹çŸ¥è¯†æŸ¥è¯¢æˆåŠŸå®Œæˆ")
        return response_data

    except Exception as e:
        logger.error(f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
        return _handle_query_error(e, start_time)

def _handle_timeout_error(query: str, start_time: float) -> QueryResponse:
    """å¤„ç†æŸ¥è¯¢è¶…æ—¶é”™è¯¯"""
    return QueryResponse(
        answer=f"æŸ¥è¯¢ã€Œ{query}ã€å¤„ç†è¶…æ—¶ï¼Œè¯·å°è¯•ï¼š\n\nâ€¢ ç®€åŒ–æŸ¥è¯¢å†…å®¹\nâ€¢ ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯\nâ€¢ ç¨åå†è¯•\n\nå¦‚é—®é¢˜æŒç»­ï¼Œè¯·è”ç³»æŠ€æœ¯æ”¯æŒã€‚",
        source_data=[{
            "tool": "timeout_handler",
            "error_type": "QueryTimeout",
            "timeout_seconds": QUERY_TIMEOUT,
            "timestamp": datetime.now().isoformat()
        }],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def _preprocess_query(query: str) -> str:
    """æŸ¥è¯¢é¢„å¤„ç†å’Œä¼˜åŒ–"""
    try:
        # å»é™¤å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        processed = query.strip()
        
        # æ ‡å‡†åŒ–å¸¸è§äººäº‹æœ¯è¯­
        hr_term_mapping = {
            'å·¥èµ„': 'è–ªèµ„',
            'è–ªæ°´': 'è–ªèµ„',
            'å¹´å‡': 'å¹´ä¼‘å‡',
            'ç—…å‡': 'ç—…äº‹å‡',
            'è¿Ÿåˆ°': 'è€ƒå‹¤',
            'æ—©é€€': 'è€ƒå‹¤',
            'æ–°äºº': 'æ–°å‘˜å·¥',
            'ç¦»èŒæ‰‹ç»­': 'ç¦»èŒæµç¨‹'
        }
        
        for old_term, new_term in hr_term_mapping.items():
            processed = processed.replace(old_term, new_term)
        
        # æ·»åŠ æŸ¥è¯¢æ‰©å±•ï¼ˆåŒä¹‰è¯ï¼‰
        if len(processed) < 10:  # çŸ­æŸ¥è¯¢éœ€è¦æ‰©å±•
            expansion_map = {
                'è–ªèµ„': 'è–ªèµ„ å·¥èµ„ è–ªé…¬',
                'è€ƒå‹¤': 'è€ƒå‹¤ æ‰“å¡ å‡ºå‹¤',
                'è¯·å‡': 'è¯·å‡ ä¼‘å‡ å‡æœŸ',
                'åŸ¹è®­': 'åŸ¹è®­ å­¦ä¹  å‘å±•'
            }
            
            for key, expansion in expansion_map.items():
                if key in processed:
                    processed = expansion
                    break
        
        return processed
        
    except Exception as e:
        logger.warning(f"æŸ¥è¯¢é¢„å¤„ç†å¤±è´¥: {e}")
        return query

def _execute_vector_search(query: str, query_analysis: dict, user_ctx: UserContext) -> list:
    """æ‰§è¡Œå‘é‡æœç´¢ - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        if not vector_manager:
            logger.error("å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []

        # æ™ºèƒ½æœç´¢å‚æ•°è°ƒæ•´
        confidence = query_analysis.get('confidence', 0.0)
        primary_intent = query_analysis.get('primary_intent', 'general')
        
        # æ ¹æ®æ„å›¾å’Œç½®ä¿¡åº¦è°ƒæ•´æœç´¢å‚æ•°
        if confidence > 0.7:
            search_k = 8  # é«˜ç½®ä¿¡åº¦ï¼Œå¤šæ£€ç´¢
        elif confidence > 0.3:
            search_k = 5  # ä¸­ç­‰ç½®ä¿¡åº¦ï¼Œæ ‡å‡†æ£€ç´¢
        else:
            search_k = 3  # ä½ç½®ä¿¡åº¦ï¼Œå°‘æ£€ç´¢

        # æ„å»ºæ™ºèƒ½è¿‡æ»¤æ¡ä»¶
        filter_metadata = _build_search_filters(primary_intent, user_ctx)

        # æ‰§è¡Œå‘é‡æœç´¢
        results = vector_manager.search_documents(
            query=query,
            k=search_k,
            filter_metadata=filter_metadata
        )
        
        if results is None:
            results = []
            
        logger.info(f"å‘é‡æœç´¢å®Œæˆ: æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        return results
        
    except Exception as e:
        logger.error(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return []

def _build_search_filters(primary_intent: str, user_ctx: UserContext) -> dict:
    """æ„å»ºæœç´¢è¿‡æ»¤æ¡ä»¶"""
    filter_metadata = {}
    
    # æš‚æ—¶ç¦ç”¨åˆ†ç±»è¿‡æ»¤ï¼Œå› ä¸ºä¸Šä¼ çš„æ–‡æ¡£å¯èƒ½ä½¿ç”¨ä¸åŒçš„åˆ†ç±»
    # æ ¹æ®æ„å›¾è®¾ç½®åˆ†ç±»è¿‡æ»¤
    # intent_category_map = {
    #     'policy': 'æ”¿ç­–åˆ¶åº¦',
    #     'process': 'æµç¨‹æŒ‡å—', 
    #     'benefit': 'ç¦åˆ©å¾…é‡',
    #     'attendance': 'è€ƒå‹¤ç®¡ç†',
    #     'salary': 'è–ªé…¬ç®¡ç†',
    #     'training': 'åŸ¹è®­èµ„æ–™',
    #     'onboarding': 'å…¥èŒæŒ‡å—',
    #     'offboarding': 'ç¦»èŒæŒ‡å—'
    # }
    # 
    # if primary_intent in intent_category_map:
    #     filter_metadata['category'] = intent_category_map[primary_intent]
    
    # æ ¹æ®ç”¨æˆ·è§’è‰²è®¾ç½®è®¿é—®çº§åˆ«è¿‡æ»¤
    role_access_map = {
        'employee': ['public', 'internal'],
        'hr_staff': ['public', 'internal', 'confidential'],
        'hr_manager': ['public', 'internal', 'confidential', 'restricted'],
        'hr_director': ['public', 'internal', 'confidential', 'restricted', 'secret']
    }
    
    user_role = user_ctx.user_role
    if user_role in role_access_map:
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å‘é‡æ•°æ®åº“æ”¯æŒ $in æ“ä½œç¬¦
        # å¦‚æœä¸æ”¯æŒï¼Œå¯ä»¥å»æ‰è¿™ä¸ªè¿‡æ»¤æ¡ä»¶
        pass  # æš‚æ—¶ä¸ä½¿ç”¨è®¿é—®çº§åˆ«è¿‡æ»¤ï¼Œé¿å…å…¼å®¹æ€§é—®é¢˜
    
    return filter_metadata if filter_metadata else None

def _build_user_context(user_ctx: UserContext) -> dict:
    """æ„å»ºç”¨æˆ·ä¸Šä¸‹æ–‡å­—å…¸"""
    return {
        'user_role': user_ctx.user_role,
        'department': user_ctx.department,
        'user_id': user_ctx.user_id
    }

def _handle_empty_results(query: str, start_time: float) -> QueryResponse:
    """å¤„ç†ç©ºç»“æœçš„æƒ…å†µ"""
    # æä¾›æ™ºèƒ½å»ºè®®
    suggestions = _generate_query_suggestions(query)
    
    answer = f"""æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ä¸ã€Œ{query}ã€ç›´æ¥ç›¸å…³çš„äººäº‹æ”¿ç­–ä¿¡æ¯ã€‚

ã€å»ºè®®å°è¯•ã€‘
{chr(10).join(f'â€¢ {suggestion}' for suggestion in suggestions)}

ã€å…¶ä»–æ–¹å¼ã€‘
â€¢ è”ç³»äººäº‹éƒ¨é—¨è·å–æœ€æ–°æ”¿ç­–ä¿¡æ¯
â€¢ æŸ¥é˜…å…¬å¸å†…éƒ¨äººäº‹ç®¡ç†ç³»ç»Ÿ
â€¢ å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯é‡æ–°æŸ¥è¯¢"""

    return QueryResponse(
        answer=answer,
        source_data=[],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def _generate_query_suggestions(query: str) -> list:
    """ç”ŸæˆæŸ¥è¯¢å»ºè®®"""
    suggestions = []
    
    # åŸºäºå¸¸è§äººäº‹æŸ¥è¯¢çš„å»ºè®®
    common_queries = {
        'è–ªèµ„': ['è–ªèµ„å‘æ”¾æ—¶é—´', 'è–ªèµ„æ„æˆè¯´æ˜', 'è–ªèµ„è°ƒæ•´æ”¿ç­–'],
        'è¯·å‡': ['å¹´ä¼‘å‡ç”³è¯·æµç¨‹', 'ç—…å‡æ”¿ç­–', 'äº‹å‡è§„å®š'],
        'è€ƒå‹¤': ['è€ƒå‹¤åˆ¶åº¦', 'æ‰“å¡è¦æ±‚', 'è¿Ÿåˆ°æ—©é€€å¤„ç†'],
        'åŸ¹è®­': ['åŸ¹è®­è®¡åˆ’', 'åŸ¹è®­æŠ¥å', 'åŸ¹è®­è¯ä¹¦'],
        'å…¥èŒ': ['æ–°å‘˜å·¥å…¥èŒæµç¨‹', 'å…¥èŒææ–™æ¸…å•', 'è¯•ç”¨æœŸæ”¿ç­–'],
        'ç¦»èŒ': ['ç¦»èŒæ‰‹ç»­åŠç†', 'ç¦»èŒè¯æ˜', 'å·¥ä½œäº¤æ¥']
    }
    
    for keyword, related_queries in common_queries.items():
        if keyword in query:
            suggestions.extend(related_queries[:2])  # æ¯ä¸ªç±»åˆ«æœ€å¤š2ä¸ªå»ºè®®
            break
    
    if not suggestions:
        suggestions = ['å°è¯•ä½¿ç”¨æ›´å…·ä½“çš„å…³é”®è¯', 'æŸ¥è¯¢å…·ä½“çš„æ”¿ç­–åç§°', 'æè¿°å…·ä½“çš„ä¸šåŠ¡åœºæ™¯']
    
    return suggestions[:3]  # æœ€å¤š3ä¸ªå»ºè®®

def _build_response_data(answer: str, vector_results: list, query_analysis: dict, 
                        original_query: str, processed_query: str, start_time: float,
                        evaluation: dict = None) -> QueryResponse:
    """æ„å»ºä¼˜åŒ–çš„å“åº”æ•°æ®ï¼ŒåŒ…å«æ˜ç¡®çš„æ¥æºæ ‡è¯†"""
    strategy = query_analysis.get('strategy', 'unknown')
    
    # ç¡®å®šä¿¡æ¯æ¥æºç±»å‹
    if strategy == 'document_based' and vector_results:
        source_type = "company_documents"  # å…¬å¸ä¸Šä¼ çš„æ–‡æ¡£
        source_description = "å·²ä¸Šä¼ çš„å…¬å¸æ–‡æ¡£"
    else:
        source_type = "general_knowledge"  # AIé€šç”¨çŸ¥è¯†
        source_description = "AIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰"
    
    # æ„å»ºè¯¦ç»†çš„æ¥æºæ•°æ®
    source_data = [{
        "tool": "hr_knowledge_search",
        "source_type": source_type,  # æ–°å¢ï¼šæ˜ç¡®çš„æ¥æºç±»å‹
        "source_description": source_description,  # æ–°å¢ï¼šæ¥æºæè¿°
        "original_query": original_query,
        "processed_query": processed_query,
        "strategy": strategy,
        "evaluation": evaluation or {},
        "search_results": [
            {
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {},
                "relevance_score": doc.metadata.get('score', 0.0) if hasattr(doc, 'metadata') else 0.0,
                "document_id": doc.metadata.get('document_id', '') if hasattr(doc, 'metadata') else '',
                "title": doc.metadata.get('title', '') if hasattr(doc, 'metadata') else ''
            } for doc in vector_results
        ],
        "result_count": len(vector_results),
        "document_titles": list(set([
            doc.metadata.get('title', '') 
            for doc in vector_results 
            if hasattr(doc, 'metadata') and doc.metadata.get('title')
        ]))  # æ–°å¢ï¼šå‚è€ƒçš„æ–‡æ¡£æ ‡é¢˜åˆ—è¡¨
    }]

    # ä¼˜åŒ–çš„ç½®ä¿¡åº¦è®¡ç®—
    if evaluation:
        confidence = evaluation.get('max_score', 0.5)
    else:
        confidence = _calculate_response_confidence(vector_results, query_analysis)

    return QueryResponse(
        answer=answer,
        source_data=source_data,
        confidence=confidence,
        processing_time=time.time() - start_time
    )



def _calculate_response_confidence(vector_results: list, query_analysis: dict) -> float:
    """è®¡ç®—å“åº”ç½®ä¿¡åº¦"""
    try:
        # åŸºç¡€ç½®ä¿¡åº¦ï¼šåŸºäºæ£€ç´¢åˆ°çš„æ–‡æ¡£æ•°é‡
        result_count = len(vector_results)
        base_confidence = min(result_count / 5.0, 1.0)
        
        # æ„å›¾åˆ†æç½®ä¿¡åº¦
        intent_confidence = query_analysis.get('confidence', 0.0)
        
        # æ–‡æ¡£ç›¸å…³æ€§ç½®ä¿¡åº¦
        if vector_results:
            scores = [getattr(doc, 'score', 0.0) for doc in vector_results]
            avg_score = sum(scores) / len(scores) if scores else 0.0
            relevance_confidence = min(avg_score, 1.0)
        else:
            relevance_confidence = 0.0
        
        # ç»¼åˆç½®ä¿¡åº¦è®¡ç®—
        confidence = (base_confidence * 0.4 + intent_confidence * 0.3 + relevance_confidence * 0.3)
        
        return round(confidence, 3)
        
    except Exception as e:
        logger.warning(f"ç½®ä¿¡åº¦è®¡ç®—å¤±è´¥: {e}")
        return 0.5  # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦

def _handle_query_error(error: Exception, start_time: float) -> QueryResponse:
    """å¤„ç†æŸ¥è¯¢é”™è¯¯"""
    error_msg = str(error)
    
    # æ ¹æ®é”™è¯¯ç±»å‹æä¾›ä¸åŒçš„ç”¨æˆ·å‹å¥½æ¶ˆæ¯
    if "timeout" in error_msg.lower():
        user_message = "æŸ¥è¯¢è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•æˆ–ç®€åŒ–æŸ¥è¯¢å†…å®¹ã€‚"
    elif "connection" in error_msg.lower():
        user_message = "ç³»ç»Ÿè¿æ¥å¼‚å¸¸ï¼Œè¯·ç¨åå†è¯•ã€‚"
    elif "memory" in error_msg.lower():
        user_message = "ç³»ç»Ÿèµ„æºä¸è¶³ï¼Œè¯·ç¨åå†è¯•ã€‚"
    else:
        user_message = "ç³»ç»Ÿå¤„ç†å‡ºç°é—®é¢˜ï¼Œè¯·ç¨åå†è¯•æˆ–è”ç³»æŠ€æœ¯æ”¯æŒã€‚"
    
    return QueryResponse(
        answer=user_message,
        source_data=[{
            "tool": "error_handler",
            "error_type": type(error).__name__,
            "error_message": error_msg,
            "timestamp": datetime.now().isoformat()
        }],
        confidence=0.0,
        processing_time=time.time() - start_time
    )

def service_upload_document(file, title: str, category: str, access_level: str, user_ctx: str) -> DocumentUploadResponse:
    """å¤„ç†æ–‡æ¡£ä¸Šä¼ è¯·æ±‚ - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        logger.info(f"æ”¶åˆ°æ–‡æ¡£ä¸Šä¼ : {file.filename} (æ ‡é¢˜: {title})")

        if vector_manager is None:
            logger.error('å‘é‡å­˜å‚¨ç®¡ç†å™¨æœªåˆå§‹åŒ–')
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        user_context = json.loads(user_ctx)

        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦é‡å¤
        if vector_manager.check_duplicate_title(title):
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£æ ‡é¢˜ '{title}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜")

        try:
            document_chunks = process_upload_file(file)
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

        # ç”Ÿæˆæ–‡æ¡£ID
        doc_id_base = f"hr_doc_{int(time.time())}"
        document_id = doc_id_base

        # ä¼˜åŒ–çš„å…ƒæ•°æ®ç»“æ„ï¼Œé€‚é…äººäº‹åœºæ™¯
        metadatas = [{
            "document_id": document_id,
            "doc_type": "uploaded",
            "access_level": access_level,
            "title": title,
            "category": category,
            "source_file": file.filename,
            "department": user_context.get("department", "HR"),
            "uploader": user_context.get("user_role", "hr_staff"),
            "upload_time": datetime.now().isoformat(),
            "chunk_index": i,
            "total_chunks": len(document_chunks)
        } for i, _ in enumerate(document_chunks)]

        chunk_ids = [f"{doc_id_base}_chunk_{i}" for i in range(len(document_chunks))]

        # ä½¿ç”¨ä¼˜åŒ–çš„å‘é‡å­˜å‚¨ç®¡ç†å™¨
        success = vector_manager.add_document(
            texts=document_chunks,
            metadatas=metadatas,
            ids=chunk_ids
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ–‡æ¡£æ·»åŠ åˆ°å‘é‡æ•°æ®åº“å¤±è´¥")

        logger.info(f"æˆåŠŸæ·»åŠ  {len(document_chunks)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")

        return DocumentUploadResponse(
            status="success",
            document_id=document_id,
            filename=file.filename,
            chunks=len(document_chunks),
            first_chunk=document_chunks[0][:100] + "..." if document_chunks else "",
            message=f"æ–‡æ¡£ '{title}' ä¸Šä¼ æˆåŠŸï¼Œå·²åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªæ–‡æ¡£å—"
        )
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£ä¸Šä¼ å¤±è´¥: {str(e)}")

def service_delete_document(document_id: str) -> dict:
    """åˆ é™¤æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        success = vector_manager.delete_document(document_id)
        
        if success:
            return {
                "status": "success",
                "message": f"æ–‡æ¡£ {document_id} åˆ é™¤æˆåŠŸ"
            }
        else:
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ {document_id} ä¸å­˜åœ¨")
            
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_update_document(document_id: str, file, title: str, category: str, access_level: str, user_ctx: str) -> DocumentUploadResponse:
    """æ›´æ–°æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        user_context = json.loads(user_ctx)

        # æ£€æŸ¥æ ‡é¢˜æ˜¯å¦ä¸å…¶ä»–æ–‡æ¡£é‡å¤ï¼ˆæ’é™¤å½“å‰æ–‡æ¡£ï¼‰
        if vector_manager.check_duplicate_title(title, exclude_doc_id=document_id):
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£æ ‡é¢˜ '{title}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„æ ‡é¢˜")

        try:
            document_chunks = process_upload_file(file)
        except Exception as e:
            logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}", exc_info=True)
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")

        # ä½¿ç”¨åŸæœ‰çš„æ–‡æ¡£ID
        metadatas = [{
            "document_id": document_id,
            "doc_type": "uploaded",
            "access_level": access_level,
            "title": title,
            "category": category,
            "source_file": file.filename,
            "department": user_context.get("department", "HR"),
            "uploader": user_context.get("user_role", "hr_staff"),
            "upload_time": datetime.now().isoformat(),
            "chunk_index": i,
            "total_chunks": len(document_chunks)
        } for i, _ in enumerate(document_chunks)]

        chunk_ids = [f"{document_id}_chunk_{i}" for i in range(len(document_chunks))]

        success = vector_manager.update_document(
            document_id=document_id,
            texts=document_chunks,
            metadatas=metadatas,
            ids=chunk_ids
        )

        if not success:
            raise HTTPException(status_code=500, detail="æ–‡æ¡£æ›´æ–°å¤±è´¥")

        return DocumentUploadResponse(
            status="success",
            document_id=document_id,
            filename=file.filename,
            chunks=len(document_chunks),
            first_chunk=document_chunks[0][:100] + "..." if document_chunks else "",
            message=f"æ–‡æ¡£ '{title}' æ›´æ–°æˆåŠŸï¼Œå·²åˆ†å‰²ä¸º {len(document_chunks)} ä¸ªæ–‡æ¡£å—"
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_list_documents(limit: Optional[int] = None) -> List[DocumentInfo]:
    """è·å–æ–‡æ¡£åˆ—è¡¨"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        documents = vector_manager.list_documents(limit=limit)
        
        # è½¬æ¢ä¸ºDocumentInfoæ ¼å¼
        doc_infos = []
        for doc in documents:
            # å®‰å…¨åœ°è½¬æ¢æšä¸¾ç±»å‹
            category_str = doc.get('category', 'å…¶ä»–')
            access_level_str = doc.get('access_level', 'å…¨å‘˜')
            
            # æŸ¥æ‰¾åŒ¹é…çš„æšä¸¾å€¼
            category = DocumentCategory.OTHER
            for cat in DocumentCategory:
                if cat.value == category_str:
                    category = cat
                    break
            
            access_level = AccessLevel.PUBLIC
            for level in AccessLevel:
                if level.value == access_level_str:
                    access_level = level
                    break
            
            doc_info = DocumentInfo(
                id=doc.get('document_id', ''),
                title=doc.get('title', ''),
                category=category,
                access_level=access_level,
                filename=doc.get('source_file', ''),
                upload_time=datetime.fromisoformat(doc.get('upload_time', datetime.now().isoformat())),
                uploader=doc.get('uploader', ''),
                chunks_count=doc.get('total_chunks', 0)
            )
            doc_infos.append(doc_info)
        
        return doc_infos
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {str(e)}")

def service_search_documents(query: str, category: Optional[str] = None, access_level: Optional[str] = None, k: int = 5) -> List[dict]:
    """æœç´¢æ–‡æ¡£"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        # æ„å»ºè¿‡æ»¤æ¡ä»¶
        filter_metadata = {}
        if category:
            filter_metadata['category'] = category
        if access_level:
            filter_metadata['access_level'] = access_level

        results = vector_manager.search_documents(
            query=query,
            k=k,
            filter_metadata=filter_metadata if filter_metadata else None
        )

        # æ ¼å¼åŒ–æœç´¢ç»“æœ
        formatted_results = []
        for doc in results:
            formatted_results.append({
                "content": doc.page_content,
                "metadata": doc.metadata if hasattr(doc, 'metadata') else {},
                "score": getattr(doc, 'score', 0.0)
            })

        return formatted_results
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æœç´¢æ–‡æ¡£å¤±è´¥: {str(e)}")

def service_health_check() -> HealthResponse:
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    # æ£€æŸ¥å„ç»„ä»¶çŠ¶æ€
    vector_db_status = "active" if vector_store else "inactive"
    
    # å¦‚æœå…³é”®ç»„ä»¶ä¸å¯ç”¨ï¼Œç³»ç»ŸçŠ¶æ€ä¸ºè­¦å‘Š
    system_status = "ok" if vector_store else "warning"
    
    detail = {
        "version": "2.0",
        "mode": "api" if vector_store else "simplified",
        "components": {
            "vector_db": vector_db_status,
            "llm_integration": "alibaba_cloud",
            "api_server": "active",
            "query_router": "active" if query_router else "inactive"
        }
    }
    
    # å¦‚æœæ˜¯ç®€åŒ–æ¨¡å¼ï¼Œæ·»åŠ è¯´æ˜
    if not vector_store:
        detail["notice"] = "ç³»ç»Ÿè¿è¡Œåœ¨ç®€åŒ–æ¨¡å¼ï¼Œå‘é‡æœç´¢åŠŸèƒ½ä¸å¯ç”¨ã€‚"
    
    return HealthResponse(
        status=system_status,
        service="äººäº‹çŸ¥è¯†åº“ç³»ç»Ÿ",
        detail=detail
    )

def service_vector_status() -> VectorStoreStatus:
    """è·å–å‘é‡å­˜å‚¨çŠ¶æ€ - ä¼˜åŒ–ç‰ˆæœ¬"""
    try:
        if vector_manager:
            stats = vector_manager.get_collection_stats()
            
            return VectorStoreStatus(
                status="active",
                documents=stats.get('total_documents', 0),
                collection_name=stats.get('collection_name', 'hr_knowledge'),
                last_updated=datetime.fromisoformat(stats.get('last_updated', datetime.now().isoformat()))
            )
        else:
            return VectorStoreStatus(
                status="not_initialized",
                documents=0,
                collection_name="hr_knowledge"
            )
    except Exception as e:
        logger.error(f"å‘é‡æ•°æ®åº“çŠ¶æ€æ£€æŸ¥å¤±è´¥: {str(e)}")
        return VectorStoreStatus(
            status="error",
            documents=0,
            collection_name="hr_knowledge"
        )

def service_get_collection_stats() -> dict:
    """è·å–å‘é‡å­˜å‚¨è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
    try:
        if vector_manager is None:
            raise HTTPException(status_code=500, detail="å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")

        stats = vector_manager.get_collection_stats()
        return {
            "status": "success",
            "data": stats
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {str(e)}")

async def service_query_knowledge_stream(request: QueryRequest) -> AsyncGenerator[dict, None]:
    """å¤„ç†äººäº‹çŸ¥è¯†æŸ¥è¯¢è¯·æ±‚ - æµå¼ç‰ˆæœ¬ï¼ˆä½¿ç”¨æ™ºèƒ½è·¯ç”±ï¼‰"""
    start_time = time.time()
    logger.info(f"æ”¶åˆ°æµå¼æŸ¥è¯¢: {request.question}")

    try:
        if not query_router or not vector_store:
            yield {
                'type': 'error',
                'message': 'ç³»ç»Ÿæœªæ­£ç¡®åˆå§‹åŒ–',
                'error_type': 'SystemError'
            }
            return
        
        # 1. å‘é€æŸ¥è¯¢é¢„å¤„ç†çŠ¶æ€
        yield {
            'type': 'status',
            'stage': 'preprocessing',
            'message': 'æ­£åœ¨é¢„å¤„ç†æŸ¥è¯¢...',
            'progress': 10
        }
        
        # æŸ¥è¯¢é¢„å¤„ç†
        processed_query = _preprocess_query(request.question)
        logger.info(f"æŸ¥è¯¢é¢„å¤„ç†å®Œæˆ: {processed_query}")
        
        # 2. å‘é€è·¯ç”±åˆ†æçŠ¶æ€
        yield {
            'type': 'status',
            'stage': 'routing',
            'message': 'æ­£åœ¨åˆ†ææŸ¥è¯¢å¹¶è·¯ç”±...',
            'progress': 30
        }

        # ä½¿ç”¨æŸ¥è¯¢è·¯ç”±å™¨
        user_ctx_dict = _build_user_context(request.user_ctx)
        route_result = query_router.route(processed_query, vector_store, user_ctx_dict)
        
        strategy = route_result['strategy']
        documents = route_result['documents']
        evaluation = route_result['evaluation']
        
        logger.info(f"è·¯ç”±ç­–ç•¥: {strategy}, æ–‡æ¡£æ•°: {len(documents)}")
        
        # 3. å‘é€LLMç”Ÿæˆå¼€å§‹çŠ¶æ€
        yield {
            'type': 'status',
            'stage': 'llm_generation',
            'message': f'æ­£åœ¨ç”Ÿæˆå›ç­”ï¼ˆç­–ç•¥: {strategy}ï¼‰...',
            'progress': 50
        }

        # 4. æµå¼ç”ŸæˆLLMå›ç­”
        logger.info("å¼€å§‹æµå¼LLMå›ç­”ç”Ÿæˆ...")
        
        full_answer = ""
        chunk_count = 0
        async for chunk in _generate_streaming_response(documents, processed_query, user_ctx_dict):
            full_answer += chunk
            chunk_count += 1
            logger.info(f"[Stream] Chunk #{chunk_count}, length: {len(chunk)}, total: {len(full_answer)}")
            yield {
                'type': 'content',
                'content': chunk,
                'is_complete': False,
                'progress': min(90, 50 + (len(full_answer) / 10))
            }
        
        logger.info("æµå¼LLMå›ç­”ç”Ÿæˆå®Œæˆ")

        # 5. å‘é€å®ŒæˆçŠ¶æ€
        processing_time = time.time() - start_time
        confidence = evaluation.get('max_score', 0.5)
        
        yield {
            'type': 'complete',
            'message': 'å›ç­”ç”Ÿæˆå®Œæˆ',
            'progress': 100,
            'full_answer': full_answer,
            'confidence': confidence,
            'processing_time': processing_time,
            'source_count': len(documents),
            'strategy': strategy
        }

        logger.info("æµå¼äººäº‹çŸ¥è¯†æŸ¥è¯¢æˆåŠŸå®Œæˆ")

    except Exception as e:
        logger.error(f"æµå¼æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}")
        yield {
            'type': 'error',
            'message': f"æŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}",
            'error_type': type(e).__name__
        }

async def _generate_streaming_response(vector_results: List, query: str, user_ctx: Dict) -> AsyncGenerator[str, None]:
    """ç”Ÿæˆæµå¼LLMå“åº” - ä½¿ç”¨é˜¿é‡Œäº‘APIçš„æµå¼æ¥å£"""
    try:
        agent = get_hr_agent()
        if not agent.llm:
            yield "æŠ±æ­‰ï¼ŒAIåŠ©æ‰‹æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åå†è¯•ã€‚"
            return
        
        # åˆ¤æ–­æ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
        if vector_results:
            # åŸºäºæ–‡æ¡£çš„å›ç­” - ä¸åœ¨å¼€å¤´æ·»åŠ æ¥æºæ ‡è¯†
            # å› ä¸ºå›ç­”ä¸­çš„ã€æ–‡æ¡£ä¾æ®ã€‘éƒ¨åˆ†å·²ç»è¯´æ˜äº†æ¥æº
            context_docs = agent._format_context_documents(vector_results)
            prompt = agent._build_enhanced_prompt(query, context_docs, user_ctx)
        else:
            # é€šç”¨çŸ¥è¯†å›ç­” - å…ˆè¾“å‡ºæ¥æºæ ‡è¯†
            yield "ğŸ’¡ **ä¿¡æ¯æ¥æºï¼šAIé€šç”¨çŸ¥è¯†ï¼ˆéå…¬å¸æ–‡æ¡£ï¼‰**\n\n"
            
            prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äººäº‹çŸ¥è¯†åº“åŠ©æ‰‹ã€‚ç”¨æˆ·é—®é¢˜ï¼š{query}

è¯·åŸºäºä½ çš„é€šç”¨çŸ¥è¯†å›ç­”è¿™ä¸ªé—®é¢˜ã€‚å¦‚æœè¿™ä¸ªé—®é¢˜ä¸äººäº‹ç®¡ç†ç›¸å…³ï¼Œè¯·æä¾›ä¸“ä¸šçš„å»ºè®®ã€‚
å¦‚æœé—®é¢˜è¶…å‡ºäººäº‹é¢†åŸŸï¼Œè¯·ç¤¼è²Œåœ°è¯´æ˜ä½ ä¸»è¦è´Ÿè´£äººäº‹ç›¸å…³é—®é¢˜ã€‚

æ³¨æ„ï¼šä¸è¦åœ¨å›ç­”ä¸­é‡å¤æ·»åŠ "ä¿¡æ¯æ¥æº"æ ‡è¯†ï¼Œå› ä¸ºå·²ç»åœ¨å‰é¢æ·»åŠ è¿‡äº†ã€‚"""
        
        # ä½¿ç”¨é˜¿é‡Œäº‘APIçš„æµå¼ç”Ÿæˆ
        try:
            logger.info("å¼€å§‹ä½¿ç”¨æµå¼APIç”Ÿæˆå›ç­”...")
            
            # ä½¿ç”¨ astream è¿›è¡Œå¼‚æ­¥æµå¼ç”Ÿæˆ
            async for chunk in agent.llm.astream(prompt):
                if chunk.content:
                    # è¿‡æ»¤æ‰LLMå¯èƒ½é‡å¤æ·»åŠ çš„æ¥æºæ ‡è¯†
                    content = chunk.content
                    if "ä¿¡æ¯æ¥æº" not in content or content.index("ä¿¡æ¯æ¥æº") > 10:
                        # ç›´æ¥è¾“å‡ºæ¯ä¸ª chunkï¼Œå®ç°çœŸæ­£çš„æµå¼æ•ˆæœ
                        yield content
                    
        except Exception as e:
            logger.error(f"LLMæµå¼ç”Ÿæˆå¤±è´¥: {str(e)}")
            yield f"\n\næŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é—®é¢˜: {str(e)}"
            
    except Exception as e:
        logger.error(f"æµå¼å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}"